True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           0                   Model:              Koopa               

[1mData Loader[0m
  Data:               job_demand_region   Root Path:          ../../dataset/demand/
  Data Path:          region.parquet      Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        .cache/checkpoints  

[1mForecasting Task[0m
  Seq Len:            6                   Label Len:          1                   
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             16345               Dec In:             16345               
  C Out:              16345               d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           2                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              learned             Activation:         gelu                
  Output Attention:   0                   

[1mRun Parameters[0m
  Num Workers:        32                  Itr:                1                   
  Train Epochs:       20                  Batch Size:         1                   
  Patience:           3                   Learning Rate:      0.0001              
  Des:                test                Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            0                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1                 

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use CPU
train 19
>>>>>>>start training : long_term_forecast_low_0_Koopa_job_demand_region_ftM_sl6_ll1_pl3_dm512_nh8_el2_dl2_df2048_expand2_dc4_fc1_eblearned_dtTrue_test_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 19
val 1
test 4
  0%|          | 0/19 [00:00<?, ?it/s]loss:23.300020217895508:   0%|          | 0/19 [00:15<?, ?it/s]loss:23.300020217895508:   5%|â–Œ         | 1/19 [00:15<04:42, 15.68s/it]loss:22.9642333984375:   5%|â–Œ         | 1/19 [00:33<04:42, 15.68s/it]  loss:22.9642333984375:  11%|â–ˆ         | 2/19 [00:33<04:44, 16.71s/it]loss:23.231252670288086:  11%|â–ˆ         | 2/19 [00:48<04:44, 16.71s/it]loss:23.231252670288086:  16%|â–ˆâ–Œ        | 3/19 [00:48<04:17, 16.07s/it]loss:23.291126251220703:  16%|â–ˆâ–Œ        | 3/19 [01:03<04:17, 16.07s/it]loss:23.291126251220703:  21%|â–ˆâ–ˆ        | 4/19 [01:03<03:56, 15.77s/it]loss:23.108810424804688:  21%|â–ˆâ–ˆ        | 4/19 [01:19<03:56, 15.77s/it]loss:23.108810424804688:  26%|â–ˆâ–ˆâ–‹       | 5/19 [01:19<03:40, 15.75s/it]loss:22.926097869873047:  26%|â–ˆâ–ˆâ–‹       | 5/19 [01:38<03:40, 15.75s/it]loss:22.926097869873047:  32%|â–ˆâ–ˆâ–ˆâ–      | 6/19 [01:38<03:40, 16.97s/it]loss:23.343767166137695:  32%|â–ˆâ–ˆâ–ˆâ–      | 6/19 [01:58<03:40, 16.97s/it]loss:23.343767166137695:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 7/19 [01:58<03:34, 17.87s/it]loss:23.779584884643555:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 7/19 [02:17<03:34, 17.87s/it]loss:23.779584884643555:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 8/19 [02:17<03:21, 18.34s/it]loss:23.30368995666504:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 8/19 [02:36<03:21, 18.34s/it] loss:23.30368995666504:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 9/19 [02:36<03:04, 18.44s/it]loss:23.66048812866211:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 9/19 [02:53<03:04, 18.44s/it]loss:23.66048812866211:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 10/19 [02:53<02:41, 17.92s/it]loss:23.69300079345703:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 10/19 [03:13<02:41, 17.92s/it]loss:23.69300079345703:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 11/19 [03:13<02:27, 18.48s/it]loss:23.259599685668945:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 11/19 [03:29<02:27, 18.48s/it]loss:23.259599685668945:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 12/19 [03:29<02:04, 17.75s/it]loss:23.217529296875:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 12/19 [03:45<02:04, 17.75s/it]   loss:23.217529296875:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 13/19 [03:45<01:43, 17.19s/it]loss:23.51638412475586:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 13/19 [04:01<01:43, 17.19s/it]loss:23.51638412475586:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 14/19 [04:01<01:24, 16.85s/it]loss:23.242889404296875:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 14/19 [04:18<01:24, 16.85s/it]loss:23.242889404296875:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 15/19 [04:18<01:08, 17.07s/it]loss:23.23067283630371:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 15/19 [04:34<01:08, 17.07s/it] loss:23.23067283630371:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 16/19 [04:34<00:50, 16.72s/it]loss:23.147541046142578:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 16/19 [04:50<00:50, 16.72s/it]loss:23.147541046142578:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 17/19 [04:50<00:32, 16.50s/it]loss:23.366140365600586:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 17/19 [05:06<00:32, 16.50s/it]loss:23.366140365600586:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 18/19 [05:06<00:16, 16.22s/it]loss:23.608959197998047:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 18/19 [05:21<00:16, 16.22s/it]loss:23.608959197998047: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [05:21<00:00, 16.12s/it]loss:23.608959197998047: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [05:22<00:00, 16.95s/it]
Epoch: 1 cost time: 322.603479385376
Epoch: 1, Steps: 19 | Train Loss: 23.3258836 Vali Loss: 3.3455036 Test Loss: 44.8290253
Validation loss decreased (inf --> 3.345504).  Saving model ...
Updating learning rate to 0.0001
  0%|          | 0/19 [00:00<?, ?it/s]loss:23.60318946838379:   0%|          | 0/19 [00:15<?, ?it/s]loss:23.60318946838379:   5%|â–Œ         | 1/19 [00:15<04:35, 15.33s/it]loss:23.287498474121094:   5%|â–Œ         | 1/19 [00:31<04:35, 15.33s/it]loss:23.287498474121094:  11%|â–ˆ         | 2/19 [00:31<04:26, 15.69s/it]loss:23.789417266845703:  11%|â–ˆ         | 2/19 [00:47<04:26, 15.69s/it]loss:23.789417266845703:  16%|â–ˆâ–Œ        | 3/19 [00:47<04:12, 15.79s/it]loss:23.412403106689453:  16%|â–ˆâ–Œ        | 3/19 [01:02<04:12, 15.79s/it]loss:23.412403106689453:  21%|â–ˆâ–ˆ        | 4/19 [01:02<03:54, 15.62s/it]loss:23.309545516967773:  21%|â–ˆâ–ˆ        | 4/19 [01:17<03:54, 15.62s/it]loss:23.309545516967773:  26%|â–ˆâ–ˆâ–‹       | 5/19 [01:17<03:35, 15.36s/it]loss:23.150053024291992:  26%|â–ˆâ–ˆâ–‹       | 5/19 [01:32<03:35, 15.36s/it]loss:23.150053024291992:  32%|â–ˆâ–ˆâ–ˆâ–      | 6/19 [01:32<03:17, 15.19s/it]loss:23.37836456298828:  32%|â–ˆâ–ˆâ–ˆâ–      | 6/19 [01:47<03:17, 15.19s/it] loss:23.37836456298828:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 7/19 [01:47<03:03, 15.26s/it]loss:23.247007369995117:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 7/19 [02:02<03:03, 15.26s/it]loss:23.247007369995117:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 8/19 [02:02<02:47, 15.20s/it]loss:23.24529457092285:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 8/19 [02:17<02:47, 15.20s/it] loss:23.24529457092285:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 9/19 [02:17<02:30, 15.09s/it]loss:23.101924896240234:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 9/19 [02:32<02:30, 15.09s/it]loss:23.101924896240234:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 10/19 [02:32<02:16, 15.16s/it]loss:23.42192840576172:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 10/19 [02:47<02:16, 15.16s/it] loss:23.42192840576172:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 11/19 [02:47<01:59, 14.98s/it]loss:23.479450225830078:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 11/19 [03:02<01:59, 14.98s/it]loss:23.479450225830078:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 12/19 [03:02<01:44, 14.97s/it]loss:23.540409088134766:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 12/19 [03:17<01:44, 14.97s/it]loss:23.540409088134766:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 13/19 [03:17<01:29, 14.92s/it]loss:23.344257354736328:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 13/19 [03:32<01:29, 14.92s/it]loss:23.344257354736328:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 14/19 [03:32<01:14, 14.99s/it]loss:23.10696029663086:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 14/19 [03:48<01:14, 14.99s/it] loss:23.10696029663086:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 15/19 [03:48<01:00, 15.23s/it]loss:23.251691818237305:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 15/19 [04:03<01:00, 15.23s/it]loss:23.251691818237305:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 16/19 [04:03<00:45, 15.12s/it]loss:23.67406463623047:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 16/19 [04:17<00:45, 15.12s/it] loss:23.67406463623047:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 17/19 [04:17<00:30, 15.04s/it]loss:23.49934196472168:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 17/19 [04:35<00:30, 15.04s/it]loss:23.49934196472168:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 18/19 [04:35<00:15, 15.90s/it]loss:23.12497901916504:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 18/19 [04:53<00:15, 15.90s/it]loss:23.12497901916504: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [04:53<00:00, 16.41s/it]loss:23.12497901916504: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [04:53<00:00, 15.45s/it]
Epoch: 2 cost time: 294.0227997303009
Epoch: 2, Steps: 19 | Train Loss: 23.3667253 Vali Loss: 3.3244519 Test Loss: 45.1758537
Validation loss decreased (3.345504 --> 3.324452).  Saving model ...
Updating learning rate to 5e-05
  0%|          | 0/19 [00:00<?, ?it/s]loss:23.479223251342773:   0%|          | 0/19 [00:17<?, ?it/s]loss:23.479223251342773:   5%|â–Œ         | 1/19 [00:17<05:17, 17.61s/it]loss:23.10988426208496:   5%|â–Œ         | 1/19 [00:37<05:17, 17.61s/it] loss:23.10988426208496:  11%|â–ˆ         | 2/19 [00:37<05:17, 18.67s/it]loss:23.75952911376953:  11%|â–ˆ         | 2/19 [00:55<05:17, 18.67s/it]loss:23.75952911376953:  16%|â–ˆâ–Œ        | 3/19 [00:55<04:57, 18.62s/it]loss:23.45490264892578:  16%|â–ˆâ–Œ        | 3/19 [01:15<04:57, 18.62s/it]loss:23.45490264892578:  21%|â–ˆâ–ˆ        | 4/19 [01:15<04:47, 19.18s/it]loss:23.474994659423828:  21%|â–ˆâ–ˆ        | 4/19 [01:33<04:47, 19.18s/it]loss:23.474994659423828:  26%|â–ˆâ–ˆâ–‹       | 5/19 [01:33<04:22, 18.72s/it]loss:23.088529586791992:  26%|â–ˆâ–ˆâ–‹       | 5/19 [01:50<04:22, 18.72s/it]loss:23.088529586791992:  32%|â–ˆâ–ˆâ–ˆâ–      | 6/19 [01:50<03:54, 18.04s/it]loss:23.386571884155273:  32%|â–ˆâ–ˆâ–ˆâ–      | 6/19 [02:05<03:54, 18.04s/it]loss:23.386571884155273:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 7/19 [02:05<03:26, 17.20s/it]loss:23.851055145263672:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 7/19 [02:21<03:26, 17.20s/it]loss:23.851055145263672:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 8/19 [02:21<03:03, 16.71s/it]loss:23.382919311523438:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 8/19 [02:36<03:03, 16.71s/it]loss:23.382919311523438:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 9/19 [02:36<02:42, 16.21s/it]loss:23.534709930419922:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 9/19 [02:51<02:42, 16.21s/it]loss:23.534709930419922:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 10/19 [02:51<02:23, 15.97s/it]loss:23.175485610961914:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 10/19 [03:08<02:23, 15.97s/it]loss:23.175485610961914:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 11/19 [03:08<02:08, 16.01s/it]loss:23.318809509277344:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 11/19 [03:22<02:08, 16.01s/it]loss:23.318809509277344:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 12/19 [03:22<01:49, 15.68s/it]loss:23.70199203491211:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 12/19 [03:38<01:49, 15.68s/it] loss:23.70199203491211:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 13/19 [03:38<01:33, 15.52s/it]loss:23.57221794128418:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 13/19 [03:53<01:33, 15.52s/it]loss:23.57221794128418:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 14/19 [03:53<01:16, 15.37s/it]loss:23.299959182739258:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 14/19 [04:07<01:16, 15.37s/it]loss:23.299959182739258:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 15/19 [04:07<01:00, 15.20s/it]loss:23.275991439819336:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 15/19 [04:23<01:00, 15.20s/it]loss:23.275991439819336:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 16/19 [04:23<00:46, 15.41s/it]loss:23.164016723632812:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 16/19 [04:39<00:46, 15.41s/it]loss:23.164016723632812:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 17/19 [04:39<00:30, 15.49s/it]loss:23.458654403686523:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 17/19 [04:54<00:30, 15.49s/it]loss:23.458654403686523:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 18/19 [04:54<00:15, 15.45s/it]loss:23.403444290161133:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 18/19 [05:10<00:15, 15.45s/it]loss:23.403444290161133: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [05:10<00:00, 15.58s/it]loss:23.403444290161133: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [05:10<00:00, 16.36s/it]
Epoch: 3 cost time: 311.4028334617615
Epoch: 3, Steps: 19 | Train Loss: 23.4154153 Vali Loss: 3.3310323 Test Loss: 44.8060226
EarlyStopping counter: 1 out of 3
Updating learning rate to 2.5e-05
  0%|          | 0/19 [00:00<?, ?it/s]loss:23.466938018798828:   0%|          | 0/19 [00:16<?, ?it/s]loss:23.466938018798828:   5%|â–Œ         | 1/19 [00:16<04:54, 16.36s/it]loss:23.109960556030273:   5%|â–Œ         | 1/19 [00:31<04:54, 16.36s/it]loss:23.109960556030273:  11%|â–ˆ         | 2/19 [00:31<04:29, 15.83s/it]loss:23.28220558166504:  11%|â–ˆ         | 2/19 [00:47<04:29, 15.83s/it] loss:23.28220558166504:  16%|â–ˆâ–Œ        | 3/19 [00:47<04:10, 15.63s/it]loss:23.399600982666016:  16%|â–ˆâ–Œ        | 3/19 [01:03<04:10, 15.63s/it]loss:23.399600982666016:  21%|â–ˆâ–ˆ        | 4/19 [01:03<03:55, 15.70s/it]loss:23.464303970336914:  21%|â–ˆâ–ˆ        | 4/19 [01:18<03:55, 15.70s/it]loss:23.464303970336914:  26%|â–ˆâ–ˆâ–‹       | 5/19 [01:18<03:38, 15.61s/it]loss:23.317710876464844:  26%|â–ˆâ–ˆâ–‹       | 5/19 [01:33<03:38, 15.61s/it]loss:23.317710876464844:  32%|â–ˆâ–ˆâ–ˆâ–      | 6/19 [01:33<03:18, 15.28s/it]loss:23.74555778503418:  32%|â–ˆâ–ˆâ–ˆâ–      | 6/19 [01:48<03:18, 15.28s/it] loss:23.74555778503418:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 7/19 [01:48<03:03, 15.30s/it]loss:23.406606674194336:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 7/19 [02:03<03:03, 15.30s/it]loss:23.406606674194336:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 8/19 [02:03<02:48, 15.28s/it]loss:23.14295768737793:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 8/19 [02:19<02:48, 15.28s/it] loss:23.14295768737793:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 9/19 [02:19<02:33, 15.31s/it]loss:23.17542266845703:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 9/19 [02:34<02:33, 15.31s/it]loss:23.17542266845703:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 10/19 [02:34<02:18, 15.35s/it]loss:23.35128402709961:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 10/19 [02:50<02:18, 15.35s/it]loss:23.35128402709961:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 11/19 [02:50<02:03, 15.41s/it]loss:23.497270584106445:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 11/19 [03:05<02:03, 15.41s/it]loss:23.497270584106445:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 12/19 [03:05<01:47, 15.32s/it]loss:23.39617919921875:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 12/19 [03:19<01:47, 15.32s/it] loss:23.39617919921875:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 13/19 [03:19<01:30, 15.15s/it]loss:23.72728729248047:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 13/19 [03:35<01:30, 15.15s/it]loss:23.72728729248047:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 14/19 [03:35<01:16, 15.29s/it]loss:23.596364974975586:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 14/19 [03:52<01:16, 15.29s/it]loss:23.596364974975586:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 15/19 [03:52<01:03, 15.78s/it]loss:23.181379318237305:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 15/19 [04:07<01:03, 15.78s/it]loss:23.181379318237305:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 16/19 [04:07<00:46, 15.54s/it]loss:23.557233810424805:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 16/19 [04:21<00:46, 15.54s/it]loss:23.557233810424805:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 17/19 [04:21<00:30, 15.23s/it]loss:23.886207580566406:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 17/19 [04:37<00:30, 15.23s/it]loss:23.886207580566406:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 18/19 [04:37<00:15, 15.28s/it]loss:23.529193878173828:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 18/19 [04:52<00:15, 15.28s/it]loss:23.529193878173828: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [04:52<00:00, 15.18s/it]loss:23.529193878173828: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [04:52<00:00, 15.39s/it]
Epoch: 4 cost time: 292.7946891784668
Epoch: 4, Steps: 19 | Train Loss: 23.4333508 Vali Loss: 3.3351867 Test Loss: 44.7941971
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.25e-05
  0%|          | 0/19 [00:00<?, ?it/s]loss:23.399560928344727:   0%|          | 0/19 [00:15<?, ?it/s]loss:23.399560928344727:   5%|â–Œ         | 1/19 [00:15<04:34, 15.23s/it]loss:23.41436767578125:   5%|â–Œ         | 1/19 [00:31<04:34, 15.23s/it] loss:23.41436767578125:  11%|â–ˆ         | 2/19 [00:31<04:24, 15.56s/it]loss:23.891504287719727:  11%|â–ˆ         | 2/19 [00:47<04:24, 15.56s/it]loss:23.891504287719727:  16%|â–ˆâ–Œ        | 3/19 [00:47<04:13, 15.83s/it]loss:23.732769012451172:  16%|â–ˆâ–Œ        | 3/19 [01:02<04:13, 15.83s/it]loss:23.732769012451172:  21%|â–ˆâ–ˆ        | 4/19 [01:02<03:55, 15.70s/it]loss:23.29619598388672:  21%|â–ˆâ–ˆ        | 4/19 [01:17<03:55, 15.70s/it] loss:23.29619598388672:  26%|â–ˆâ–ˆâ–‹       | 5/19 [01:17<03:36, 15.48s/it]loss:23.338041305541992:  26%|â–ˆâ–ˆâ–‹       | 5/19 [01:33<03:36, 15.48s/it]loss:23.338041305541992:  32%|â–ˆâ–ˆâ–ˆâ–      | 6/19 [01:33<03:21, 15.46s/it]loss:23.154659271240234:  32%|â–ˆâ–ˆâ–ˆâ–      | 6/19 [01:48<03:21, 15.46s/it]loss:23.154659271240234:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 7/19 [01:48<03:03, 15.30s/it]loss:23.768428802490234:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 7/19 [02:03<03:03, 15.30s/it]loss:23.768428802490234:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 8/19 [02:03<02:47, 15.25s/it]loss:23.53260040283203:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 8/19 [02:19<02:47, 15.25s/it] loss:23.53260040283203:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 9/19 [02:19<02:35, 15.57s/it]loss:23.179183959960938:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 9/19 [02:34<02:35, 15.57s/it]loss:23.179183959960938:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 10/19 [02:34<02:17, 15.26s/it]loss:23.127994537353516:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 10/19 [02:49<02:17, 15.26s/it]loss:23.127994537353516:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 11/19 [02:49<02:02, 15.27s/it]loss:23.358776092529297:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 11/19 [03:04<02:02, 15.27s/it]loss:23.358776092529297:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 12/19 [03:04<01:45, 15.10s/it]loss:23.564205169677734:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 12/19 [03:18<01:45, 15.10s/it]loss:23.564205169677734:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 13/19 [03:18<01:30, 15.01s/it]loss:23.511754989624023:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 13/19 [03:33<01:30, 15.01s/it]loss:23.511754989624023:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 14/19 [03:33<01:14, 14.97s/it]loss:23.41737174987793:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 14/19 [03:51<01:14, 14.97s/it] loss:23.41737174987793:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 15/19 [03:51<01:02, 15.71s/it]loss:23.607635498046875:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 15/19 [04:06<01:02, 15.71s/it]loss:23.607635498046875:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 16/19 [04:06<00:46, 15.57s/it]loss:23.191253662109375:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 16/19 [04:21<00:46, 15.57s/it]loss:23.191253662109375:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 17/19 [04:21<00:31, 15.53s/it]loss:23.488788604736328:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 17/19 [04:36<00:31, 15.53s/it]loss:23.488788604736328:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 18/19 [04:36<00:15, 15.23s/it]loss:23.45787811279297:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 18/19 [04:51<00:15, 15.23s/it] loss:23.45787811279297: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [04:51<00:00, 15.04s/it]loss:23.45787811279297: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [04:51<00:00, 15.32s/it]
Epoch: 5 cost time: 291.6429467201233
Epoch: 5, Steps: 19 | Train Loss: 23.4438405 Vali Loss: 3.3323841 Test Loss: 44.7479210
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : long_term_forecast_low_0_Koopa_job_demand_region_ftM_sl6_ll1_pl3_dm512_nh8_el2_dl2_df2048_expand2_dc4_fc1_eblearned_dtTrue_test_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 4
test shape: (4, 1, 3, 16345) (4, 1, 3, 16345)
test shape: (4, 3, 16345) (4, 3, 16345)
mse:53091.49609375, mae:36.003028869628906, dtw:-999
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           0                   Model:              Koopa               

[1mData Loader[0m
  Data:               job_demand_r1       Root Path:          ../../dataset/demand/
  Data Path:          r1.parquet          Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        .cache/checkpoints  

[1mForecasting Task[0m
  Seq Len:            6                   Label Len:          1                   
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             32690               Dec In:             32690               
  C Out:              32690               d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           2                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              learned             Activation:         gelu                
  Output Attention:   0                   

[1mRun Parameters[0m
  Num Workers:        32                  Itr:                1                   
  Train Epochs:       20                  Batch Size:         1                   
  Patience:           3                   Learning Rate:      0.0001              
  Des:                test                Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            0                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1                 

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use CPU
train 19
>>>>>>>start training : long_term_forecast_low_0_Koopa_job_demand_r1_ftM_sl6_ll1_pl3_dm512_nh8_el2_dl2_df2048_expand2_dc4_fc1_eblearned_dtTrue_test_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 19
val 1
test 4
  0%|          | 0/19 [00:00<?, ?it/s]loss:24.265321731567383:   0%|          | 0/19 [01:09<?, ?it/s]loss:24.265321731567383:   5%|â–Œ         | 1/19 [01:09<20:50, 69.48s/it]loss:24.280195236206055:   5%|â–Œ         | 1/19 [02:14<20:50, 69.48s/it]loss:24.280195236206055:  11%|â–ˆ         | 2/19 [02:14<19:00, 67.07s/it]loss:24.44528579711914:  11%|â–ˆ         | 2/19 [03:20<19:00, 67.07s/it] loss:24.44528579711914:  16%|â–ˆâ–Œ        | 3/19 [03:20<17:46, 66.64s/it]loss:24.32465171813965:  16%|â–ˆâ–Œ        | 3/19 [04:24<17:46, 66.64s/it]loss:24.32465171813965:  21%|â–ˆâ–ˆ        | 4/19 [04:24<16:20, 65.39s/it]loss:24.470417022705078:  21%|â–ˆâ–ˆ        | 4/19 [05:26<16:20, 65.39s/it]loss:24.470417022705078:  26%|â–ˆâ–ˆâ–‹       | 5/19 [05:26<14:56, 64.06s/it]loss:24.34318733215332:  26%|â–ˆâ–ˆâ–‹       | 5/19 [06:31<14:56, 64.06s/it] loss:24.34318733215332:  32%|â–ˆâ–ˆâ–ˆâ–      | 6/19 [06:31<13:59, 64.58s/it]loss:24.311405181884766:  32%|â–ˆâ–ˆâ–ˆâ–      | 6/19 [07:39<13:59, 64.58s/it]loss:24.311405181884766:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 7/19 [07:39<13:06, 65.51s/it]loss:24.259967803955078:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 7/19 [08:43<13:06, 65.51s/it]loss:24.259967803955078:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 8/19 [08:43<11:54, 64.99s/it]loss:24.367177963256836:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 8/19 [09:48<11:54, 64.99s/it]loss:24.367177963256836:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 9/19 [09:48<10:51, 65.15s/it]loss:24.69244956970215:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 9/19 [10:54<10:51, 65.15s/it] loss:24.69244956970215:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 10/19 [10:54<09:49, 65.50s/it]loss:24.2668514251709:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 10/19 [12:02<09:49, 65.50s/it] loss:24.2668514251709:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 11/19 [12:02<08:50, 66.28s/it]loss:24.579580307006836:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 11/19 [13:06<08:50, 66.28s/it]loss:24.579580307006836:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 12/19 [13:06<07:37, 65.43s/it]loss:24.26125717163086:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 12/19 [14:11<07:37, 65.43s/it] loss:24.26125717163086:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 13/19 [14:11<06:32, 65.34s/it]loss:24.22258186340332:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 13/19 [15:13<06:32, 65.34s/it]loss:24.22258186340332:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 14/19 [15:13<05:21, 64.36s/it]loss:24.61315155029297:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 14/19 [16:20<05:21, 64.36s/it]loss:24.61315155029297:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 15/19 [16:20<04:20, 65.18s/it]loss:24.488924026489258:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 15/19 [17:25<04:20, 65.18s/it]loss:24.488924026489258:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 16/19 [17:25<03:15, 65.10s/it]loss:24.619239807128906:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 16/19 [18:30<03:15, 65.10s/it]loss:24.619239807128906:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 17/19 [18:30<02:10, 65.03s/it]loss:24.791793823242188:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 17/19 [19:31<02:10, 65.03s/it]loss:24.791793823242188:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 18/19 [19:31<01:03, 63.84s/it]loss:24.52137565612793:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 18/19 [20:36<01:03, 63.84s/it] loss:24.52137565612793: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [20:36<00:00, 64.28s/it]loss:24.52137565612793: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [20:36<00:00, 65.10s/it]
Epoch: 1 cost time: 1237.4951870441437
Epoch: 1, Steps: 19 | Train Loss: 24.4276218 Vali Loss: 3.9947336 Test Loss: 68.8214417
Validation loss decreased (inf --> 3.994734).  Saving model ...
Updating learning rate to 0.0001
  0%|          | 0/19 [00:00<?, ?it/s]loss:24.496389389038086:   0%|          | 0/19 [01:06<?, ?it/s]loss:24.496389389038086:   5%|â–Œ         | 1/19 [01:06<20:03, 66.84s/it]loss:24.462753295898438:   5%|â–Œ         | 1/19 [02:17<20:03, 66.84s/it]loss:24.462753295898438:  11%|â–ˆ         | 2/19 [02:17<19:32, 68.97s/it]loss:24.434659957885742:  11%|â–ˆ         | 2/19 [03:23<19:32, 68.97s/it]loss:24.434659957885742:  16%|â–ˆâ–Œ        | 3/19 [03:23<18:05, 67.86s/it]loss:24.366065979003906:  16%|â–ˆâ–Œ        | 3/19 [04:31<18:05, 67.86s/it]loss:24.366065979003906:  21%|â–ˆâ–ˆ        | 4/19 [04:31<16:55, 67.71s/it]loss:24.637691497802734:  21%|â–ˆâ–ˆ        | 4/19 [05:33<16:55, 67.71s/it]loss:24.637691497802734:  26%|â–ˆâ–ˆâ–‹       | 5/19 [05:33<15:22, 65.87s/it]loss:24.426969528198242:  26%|â–ˆâ–ˆâ–‹       | 5/19 [06:37<15:22, 65.87s/it]loss:24.426969528198242:  32%|â–ˆâ–ˆâ–ˆâ–      | 6/19 [06:37<14:04, 64.98s/it]loss:24.748554229736328:  32%|â–ˆâ–ˆâ–ˆâ–      | 6/19 [07:37<14:04, 64.98s/it]loss:24.748554229736328:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 7/19 [07:37<12:42, 63.51s/it]loss:24.383831024169922:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 7/19 [08:40<12:42, 63.51s/it]loss:24.383831024169922:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 8/19 [08:40<11:37, 63.38s/it]loss:24.836658477783203:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 8/19 [09:46<11:37, 63.38s/it]loss:24.836658477783203:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 9/19 [09:46<10:40, 64.08s/it]loss:24.369871139526367:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 9/19 [10:49<10:40, 64.08s/it]loss:24.369871139526367:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 10/19 [10:49<09:33, 63.69s/it]loss:24.653671264648438:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 10/19 [11:50<09:33, 63.69s/it]loss:24.653671264648438:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 11/19 [11:50<08:22, 62.83s/it]loss:24.577184677124023:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 11/19 [12:53<08:22, 62.83s/it]loss:24.577184677124023:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 12/19 [12:53<07:19, 62.85s/it]loss:24.604650497436523:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 12/19 [13:54<07:19, 62.85s/it]loss:24.604650497436523:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 13/19 [13:54<06:14, 62.41s/it]loss:24.390182495117188:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 13/19 [14:58<06:14, 62.41s/it]loss:24.390182495117188:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 14/19 [14:58<05:13, 62.80s/it]loss:24.53076171875:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 14/19 [15:59<05:13, 62.80s/it]    loss:24.53076171875:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 15/19 [15:59<04:09, 62.38s/it]loss:24.475236892700195:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 15/19 [17:00<04:09, 62.38s/it]loss:24.475236892700195:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 16/19 [17:00<03:05, 61.99s/it]loss:24.443302154541016:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 16/19 [18:04<03:05, 61.99s/it]loss:24.443302154541016:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 17/19 [18:04<02:04, 62.47s/it]loss:24.582786560058594:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 17/19 [19:13<02:04, 62.47s/it]loss:24.582786560058594:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 18/19 [19:13<01:04, 64.51s/it]loss:24.856401443481445:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 18/19 [20:19<01:04, 64.51s/it]loss:24.856401443481445: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [20:19<00:00, 65.08s/it]loss:24.856401443481445: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [20:19<00:00, 64.21s/it]
Epoch: 2 cost time: 1220.5746219158173
Epoch: 2, Steps: 19 | Train Loss: 24.5409275 Vali Loss: 3.9900022 Test Loss: 68.5380936
Validation loss decreased (3.994734 --> 3.990002).  Saving model ...
Updating learning rate to 5e-05
  0%|          | 0/19 [00:00<?, ?it/s]loss:24.464662551879883:   0%|          | 0/19 [01:14<?, ?it/s]loss:24.464662551879883:   5%|â–Œ         | 1/19 [01:14<22:28, 74.89s/it]loss:24.5916748046875:   5%|â–Œ         | 1/19 [02:22<22:28, 74.89s/it]  loss:24.5916748046875:  11%|â–ˆ         | 2/19 [02:22<19:57, 70.45s/it]loss:24.39940071105957:  11%|â–ˆ         | 2/19 [03:27<19:57, 70.45s/it]loss:24.39940071105957:  16%|â–ˆâ–Œ        | 3/19 [03:27<18:12, 68.31s/it]loss:24.837726593017578:  16%|â–ˆâ–Œ        | 3/19 [04:32<18:12, 68.31s/it]loss:24.837726593017578:  21%|â–ˆâ–ˆ        | 4/19 [04:32<16:42, 66.82s/it]loss:24.479812622070312:  21%|â–ˆâ–ˆ        | 4/19 [05:36<16:42, 66.82s/it]loss:24.479812622070312:  26%|â–ˆâ–ˆâ–‹       | 5/19 [05:36<15:21, 65.81s/it]loss:24.775617599487305:  26%|â–ˆâ–ˆâ–‹       | 5/19 [06:40<15:21, 65.81s/it]loss:24.775617599487305:  32%|â–ˆâ–ˆâ–ˆâ–      | 6/19 [06:40<14:05, 65.04s/it]loss:24.49566650390625:  32%|â–ˆâ–ˆâ–ˆâ–      | 6/19 [07:42<14:05, 65.04s/it] loss:24.49566650390625:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 7/19 [07:42<12:51, 64.26s/it]loss:24.487991333007812:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 7/19 [08:47<12:51, 64.26s/it]loss:24.487991333007812:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 8/19 [08:47<11:47, 64.32s/it]loss:24.442569732666016:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 8/19 [09:52<11:47, 64.32s/it]loss:24.442569732666016:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 9/19 [09:52<10:44, 64.50s/it]loss:24.648487091064453:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 9/19 [10:56<10:44, 64.50s/it]loss:24.648487091064453:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 10/19 [10:56<09:40, 64.55s/it]loss:24.361478805541992:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 10/19 [11:57<09:40, 64.55s/it]loss:24.361478805541992:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 11/19 [11:57<08:27, 63.49s/it]loss:24.44536781311035:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 11/19 [12:57<08:27, 63.49s/it] loss:24.44536781311035:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 12/19 [12:57<07:15, 62.27s/it]loss:24.590024948120117:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 12/19 [14:00<07:15, 62.27s/it]loss:24.590024948120117:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 13/19 [14:00<06:15, 62.54s/it]loss:24.42503547668457:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 13/19 [15:05<06:15, 62.54s/it] loss:24.42503547668457:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 14/19 [15:05<05:15, 63.14s/it]loss:24.59168243408203:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 14/19 [16:04<05:15, 63.14s/it]loss:24.59168243408203:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 15/19 [16:04<04:08, 62.01s/it]loss:24.719301223754883:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 15/19 [17:07<04:08, 62.01s/it]loss:24.719301223754883:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 16/19 [17:07<03:06, 62.33s/it]loss:24.72001075744629:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 16/19 [18:17<03:06, 62.33s/it] loss:24.72001075744629:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 17/19 [18:17<02:09, 64.76s/it]loss:24.43885040283203:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 17/19 [19:20<02:09, 64.76s/it]loss:24.43885040283203:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 18/19 [19:20<01:04, 64.05s/it]loss:24.85059356689453:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 18/19 [20:28<01:04, 64.05s/it]loss:24.85059356689453: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [20:28<00:00, 65.23s/it]loss:24.85059356689453: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [20:28<00:00, 64.65s/it]
Epoch: 3 cost time: 1229.3690614700317
Epoch: 3, Steps: 19 | Train Loss: 24.5666292 Vali Loss: 3.9855413 Test Loss: 68.1451492
Validation loss decreased (3.990002 --> 3.985541).  Saving model ...
Updating learning rate to 2.5e-05
  0%|          | 0/19 [00:00<?, ?it/s]loss:24.727346420288086:   0%|          | 0/19 [01:13<?, ?it/s]loss:24.727346420288086:   5%|â–Œ         | 1/19 [01:13<21:54, 73.01s/it]loss:24.363239288330078:   5%|â–Œ         | 1/19 [02:23<21:54, 73.01s/it]loss:24.363239288330078:  11%|â–ˆ         | 2/19 [02:23<20:11, 71.29s/it]loss:24.494220733642578:  11%|â–ˆ         | 2/19 [03:33<20:11, 71.29s/it]loss:24.494220733642578:  16%|â–ˆâ–Œ        | 3/19 [03:33<18:54, 70.89s/it]loss:24.63628387451172:  16%|â–ˆâ–Œ        | 3/19 [04:42<18:54, 70.89s/it] loss:24.63628387451172:  21%|â–ˆâ–ˆ        | 4/19 [04:42<17:30, 70.01s/it]loss:24.51151466369629:  21%|â–ˆâ–ˆ        | 4/19 [05:52<17:30, 70.01s/it]loss:24.51151466369629:  26%|â–ˆâ–ˆâ–‹       | 5/19 [05:52<16:22, 70.15s/it]loss:24.60847282409668:  26%|â–ˆâ–ˆâ–‹       | 5/19 [07:06<16:22, 70.15s/it]loss:24.60847282409668:  32%|â–ˆâ–ˆâ–ˆâ–      | 6/19 [07:06<15:29, 71.48s/it]loss:24.851276397705078:  32%|â–ˆâ–ˆâ–ˆâ–      | 6/19 [08:15<15:29, 71.48s/it]loss:24.851276397705078:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 7/19 [08:15<14:07, 70.62s/it]loss:24.80221939086914:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 7/19 [09:25<14:07, 70.62s/it] loss:24.80221939086914:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 8/19 [09:25<12:53, 70.36s/it]loss:24.531949996948242:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 8/19 [10:34<12:53, 70.36s/it]loss:24.531949996948242:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 9/19 [10:34<11:39, 69.96s/it]loss:24.6124324798584:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 9/19 [11:41<11:39, 69.96s/it]  loss:24.6124324798584:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 10/19 [11:41<10:21, 69.09s/it]loss:24.73337745666504:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 10/19 [12:48<10:21, 69.09s/it]loss:24.73337745666504:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 11/19 [12:48<09:06, 68.31s/it]loss:24.447223663330078:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 11/19 [13:55<09:06, 68.31s/it]loss:24.447223663330078:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 12/19 [13:55<07:56, 68.06s/it]loss:24.683717727661133:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 12/19 [15:01<07:56, 68.06s/it]loss:24.683717727661133:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 13/19 [15:01<06:44, 67.44s/it]loss:24.88041877746582:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 13/19 [16:09<06:44, 67.44s/it] loss:24.88041877746582:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 14/19 [16:09<05:37, 67.56s/it]loss:24.456878662109375:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 14/19 [17:22<05:37, 67.56s/it]loss:24.456878662109375:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 15/19 [17:22<04:36, 69.23s/it]loss:24.469898223876953:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 15/19 [18:27<04:36, 69.23s/it]loss:24.469898223876953:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 16/19 [18:27<03:23, 67.90s/it]loss:24.532066345214844:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 16/19 [19:32<03:23, 67.90s/it]loss:24.532066345214844:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 17/19 [19:32<02:14, 67.15s/it]loss:24.422086715698242:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 17/19 [20:42<02:14, 67.15s/it]loss:24.422086715698242:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 18/19 [20:42<01:07, 67.85s/it]loss:24.49098014831543:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 18/19 [21:46<01:07, 67.85s/it] loss:24.49098014831543: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [21:46<00:00, 66.78s/it]loss:24.49098014831543: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [21:46<00:00, 68.77s/it]
Epoch: 4 cost time: 1307.233232975006
Epoch: 4, Steps: 19 | Train Loss: 24.5924002 Vali Loss: 3.9825633 Test Loss: 68.0092621
Validation loss decreased (3.985541 --> 3.982563).  Saving model ...
Updating learning rate to 1.25e-05
  0%|          | 0/19 [00:00<?, ?it/s]loss:24.540708541870117:   0%|          | 0/19 [01:08<?, ?it/s]loss:24.540708541870117:   5%|â–Œ         | 1/19 [01:08<20:26, 68.13s/it]loss:24.742473602294922:   5%|â–Œ         | 1/19 [02:16<20:26, 68.13s/it]loss:24.742473602294922:  11%|â–ˆ         | 2/19 [02:16<19:18, 68.13s/it]loss:24.475082397460938:  11%|â–ˆ         | 2/19 [03:23<19:18, 68.13s/it]loss:24.475082397460938:  16%|â–ˆâ–Œ        | 3/19 [03:23<18:06, 67.89s/it]loss:24.464000701904297:  16%|â–ˆâ–Œ        | 3/19 [04:32<18:06, 67.89s/it]loss:24.464000701904297:  21%|â–ˆâ–ˆ        | 4/19 [04:32<17:03, 68.24s/it]loss:24.859657287597656:  21%|â–ˆâ–ˆ        | 4/19 [05:41<17:03, 68.24s/it]loss:24.859657287597656:  26%|â–ˆâ–ˆâ–‹       | 5/19 [05:41<15:59, 68.56s/it]loss:24.38133430480957:  26%|â–ˆâ–ˆâ–‹       | 5/19 [06:47<15:59, 68.56s/it] loss:24.38133430480957:  32%|â–ˆâ–ˆâ–ˆâ–      | 6/19 [06:47<14:40, 67.71s/it]loss:24.626564025878906:  32%|â–ˆâ–ˆâ–ˆâ–      | 6/19 [07:51<14:40, 67.71s/it]loss:24.626564025878906:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 7/19 [07:51<13:18, 66.55s/it]loss:24.617403030395508:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 7/19 [08:58<13:18, 66.55s/it]loss:24.617403030395508:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 8/19 [08:59<12:14, 66.81s/it]loss:24.536579132080078:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 8/19 [10:13<12:14, 66.81s/it]loss:24.536579132080078:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 9/19 [10:13<11:29, 68.98s/it]loss:24.806659698486328:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 9/19 [11:13<11:29, 68.98s/it]loss:24.806659698486328:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 10/19 [11:13<09:56, 66.25s/it]loss:24.541135787963867:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 10/19 [12:12<09:56, 66.25s/it]loss:24.541135787963867:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 11/19 [12:12<08:32, 64.03s/it]loss:24.89113998413086:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 11/19 [13:11<08:32, 64.03s/it] loss:24.89113998413086:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 12/19 [13:11<07:17, 62.43s/it]loss:24.497943878173828:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 12/19 [14:08<07:17, 62.43s/it]loss:24.497943878173828:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 13/19 [14:08<06:06, 61.05s/it]loss:24.52277183532715:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 13/19 [15:08<06:06, 61.05s/it] loss:24.52277183532715:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 14/19 [15:08<05:02, 60.49s/it]loss:24.658702850341797:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 14/19 [16:06<05:02, 60.49s/it]loss:24.658702850341797:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 15/19 [16:06<03:59, 59.75s/it]loss:24.75395393371582:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 15/19 [17:05<03:59, 59.75s/it] loss:24.75395393371582:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 16/19 [17:05<02:58, 59.58s/it]loss:24.426807403564453:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 16/19 [18:03<02:58, 59.58s/it]loss:24.426807403564453:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 17/19 [18:03<01:58, 59.15s/it]loss:24.703041076660156:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 17/19 [19:01<01:58, 59.15s/it]loss:24.703041076660156:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 18/19 [19:01<00:58, 58.90s/it]loss:24.467763900756836:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 18/19 [20:00<00:58, 58.90s/it]loss:24.467763900756836: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [20:00<00:00, 58.83s/it]loss:24.467763900756836: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [20:00<00:00, 63.19s/it]
Epoch: 5 cost time: 1201.27836561203
Epoch: 5, Steps: 19 | Train Loss: 24.6059854 Vali Loss: 3.9830301 Test Loss: 68.0278168
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.25e-06
  0%|          | 0/19 [00:00<?, ?it/s]loss:24.633527755737305:   0%|          | 0/19 [00:58<?, ?it/s]loss:24.633527755737305:   5%|â–Œ         | 1/19 [00:58<17:25, 58.08s/it]loss:24.701993942260742:   5%|â–Œ         | 1/19 [01:56<17:25, 58.08s/it]loss:24.701993942260742:  11%|â–ˆ         | 2/19 [01:56<16:33, 58.42s/it]loss:24.389806747436523:  11%|â–ˆ         | 2/19 [02:54<16:33, 58.42s/it]loss:24.389806747436523:  16%|â–ˆâ–Œ        | 3/19 [02:54<15:29, 58.07s/it]loss:24.62713623046875:  16%|â–ˆâ–Œ        | 3/19 [03:52<15:29, 58.07s/it] loss:24.62713623046875:  21%|â–ˆâ–ˆ        | 4/19 [03:52<14:28, 57.93s/it]loss:24.474258422851562:  21%|â–ˆâ–ˆ        | 4/19 [04:49<14:28, 57.93s/it]loss:24.474258422851562:  26%|â–ˆâ–ˆâ–‹       | 5/19 [04:49<13:27, 57.68s/it]loss:24.502853393554688:  26%|â–ˆâ–ˆâ–‹       | 5/19 [05:48<13:27, 57.68s/it]loss:24.502853393554688:  32%|â–ˆâ–ˆâ–ˆâ–      | 6/19 [05:48<12:36, 58.20s/it]loss:24.816162109375:  32%|â–ˆâ–ˆâ–ˆâ–      | 6/19 [06:46<12:36, 58.20s/it]   loss:24.816162109375:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 7/19 [06:46<11:37, 58.11s/it]loss:24.428539276123047:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 7/19 [07:44<11:37, 58.11s/it]loss:24.428539276123047:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 8/19 [07:44<10:39, 58.16s/it]loss:24.541034698486328:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 8/19 [08:43<10:39, 58.16s/it]loss:24.541034698486328:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 9/19 [08:43<09:42, 58.24s/it]loss:24.547306060791016:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 9/19 [09:40<09:42, 58.24s/it]loss:24.547306060791016:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 10/19 [09:40<08:42, 58.09s/it]loss:24.7553768157959:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 10/19 [10:39<08:42, 58.09s/it]  loss:24.7553768157959:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 11/19 [10:39<07:45, 58.15s/it]loss:24.52730369567871:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 11/19 [11:36<07:45, 58.15s/it]loss:24.52730369567871:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 12/19 [11:36<06:45, 57.92s/it]loss:24.470788955688477:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 12/19 [12:34<06:45, 57.92s/it]loss:24.470788955688477:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 13/19 [12:34<05:48, 58.05s/it]loss:24.898651123046875:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 13/19 [13:33<05:48, 58.05s/it]loss:24.898651123046875:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 14/19 [13:33<04:51, 58.21s/it]loss:24.76032066345215:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 14/19 [14:31<04:51, 58.21s/it] loss:24.76032066345215:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 15/19 [14:31<03:52, 58.09s/it]loss:24.48799705505371:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 15/19 [15:30<03:52, 58.09s/it]loss:24.48799705505371:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 16/19 [15:30<02:55, 58.51s/it]loss:24.549909591674805:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 16/19 [16:29<02:55, 58.51s/it]loss:24.549909591674805:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 17/19 [16:29<01:56, 58.44s/it]loss:24.87108039855957:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 17/19 [17:27<01:56, 58.44s/it] loss:24.87108039855957:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 18/19 [17:27<00:58, 58.29s/it]loss:24.666889190673828:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 18/19 [18:24<00:58, 58.29s/it]loss:24.666889190673828: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [18:24<00:00, 57.91s/it]loss:24.666889190673828: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [18:24<00:00, 58.11s/it]
Epoch: 6 cost time: 1104.702586889267
Epoch: 6, Steps: 19 | Train Loss: 24.6132072 Vali Loss: 3.9837332 Test Loss: 68.0443420
EarlyStopping counter: 2 out of 3
Updating learning rate to 3.125e-06
  0%|          | 0/19 [00:00<?, ?it/s]loss:24.430435180664062:   0%|          | 0/19 [00:58<?, ?it/s]loss:24.430435180664062:   5%|â–Œ         | 1/19 [00:58<17:29, 58.32s/it]loss:24.473379135131836:   5%|â–Œ         | 1/19 [01:57<17:29, 58.32s/it]loss:24.473379135131836:  11%|â–ˆ         | 2/19 [01:57<16:36, 58.61s/it]loss:24.52910804748535:  11%|â–ˆ         | 2/19 [02:55<16:36, 58.61s/it] loss:24.52910804748535:  16%|â–ˆâ–Œ        | 3/19 [02:55<15:34, 58.43s/it]loss:24.506134033203125:  16%|â–ˆâ–Œ        | 3/19 [03:54<15:34, 58.43s/it]loss:24.506134033203125:  21%|â–ˆâ–ˆ        | 4/19 [03:54<14:39, 58.62s/it]loss:24.63007354736328:  21%|â–ˆâ–ˆ        | 4/19 [04:52<14:39, 58.62s/it] loss:24.63007354736328:  26%|â–ˆâ–ˆâ–‹       | 5/19 [04:52<13:40, 58.61s/it]loss:24.75901985168457:  26%|â–ˆâ–ˆâ–‹       | 5/19 [05:51<13:40, 58.61s/it]loss:24.75901985168457:  32%|â–ˆâ–ˆâ–ˆâ–      | 6/19 [05:51<12:43, 58.76s/it]loss:24.47703742980957:  32%|â–ˆâ–ˆâ–ˆâ–      | 6/19 [06:50<12:43, 58.76s/it]loss:24.47703742980957:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 7/19 [06:50<11:43, 58.63s/it]loss:24.903371810913086:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 7/19 [07:48<11:43, 58.63s/it]loss:24.903371810913086:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 8/19 [07:48<10:44, 58.62s/it]loss:24.872758865356445:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 8/19 [08:46<10:44, 58.62s/it]loss:24.872758865356445:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 9/19 [08:46<09:43, 58.31s/it]loss:24.708152770996094:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 9/19 [09:44<09:43, 58.31s/it]loss:24.708152770996094:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 10/19 [09:44<08:44, 58.26s/it]loss:24.55215072631836:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 10/19 [10:43<08:44, 58.26s/it] loss:24.55215072631836:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 11/19 [10:43<07:48, 58.56s/it]loss:24.544673919677734:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 11/19 [11:42<07:48, 58.56s/it]loss:24.544673919677734:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 12/19 [11:42<06:49, 58.57s/it]loss:24.82106590270996:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 12/19 [12:40<06:49, 58.57s/it] loss:24.82106590270996:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 13/19 [12:40<05:49, 58.28s/it]loss:24.39422607421875:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 13/19 [13:37<05:49, 58.28s/it]loss:24.39422607421875:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 14/19 [13:37<04:50, 58.13s/it]loss:24.55109405517578:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 14/19 [14:36<04:50, 58.13s/it]loss:24.55109405517578:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 15/19 [14:36<03:53, 58.29s/it]loss:24.76466941833496:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 15/19 [15:35<03:53, 58.29s/it]loss:24.76466941833496:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 16/19 [15:35<02:55, 58.41s/it]loss:24.491701126098633:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 16/19 [16:33<02:55, 58.41s/it]loss:24.491701126098633:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 17/19 [16:33<01:56, 58.24s/it]loss:24.64093017578125:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 17/19 [17:31<01:56, 58.24s/it] loss:24.64093017578125:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 18/19 [17:31<00:58, 58.16s/it]loss:24.669775009155273:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 18/19 [18:28<00:58, 58.16s/it]loss:24.669775009155273: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [18:28<00:00, 58.07s/it]loss:24.669775009155273: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [18:28<00:00, 58.37s/it]
Epoch: 7 cost time: 1109.588610649109
Epoch: 7, Steps: 19 | Train Loss: 24.6168293 Vali Loss: 3.9835315 Test Loss: 68.0417709
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : long_term_forecast_low_0_Koopa_job_demand_r1_ftM_sl6_ll1_pl3_dm512_nh8_el2_dl2_df2048_expand2_dc4_fc1_eblearned_dtTrue_test_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 4
test shape: (4, 1, 3, 32690) (4, 1, 3, 32690)
test shape: (4, 3, 32690) (4, 3, 32690)
mse:67653.265625, mae:19.827720642089844, dtw:-999
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           0                   Model:              Koopa               

[1mData Loader[0m
  Data:               job_demand_r2       Root Path:          ../../dataset/demand/
  Data Path:          r2.parquet          Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        .cache/checkpoints  

[1mForecasting Task[0m
  Seq Len:            6                   Label Len:          1                   
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             121420              Dec In:             121420              
  C Out:              121420              d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           2                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              learned             Activation:         gelu                
  Output Attention:   0                   

[1mRun Parameters[0m
  Num Workers:        32                  Itr:                1                   
  Train Epochs:       20                  Batch Size:         1                   
  Patience:           3                   Learning Rate:      0.0001              
  Des:                test                Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            0                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1                 

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use CPU
train 19
>>>>>>>start training : long_term_forecast_low_0_Koopa_job_demand_r2_ftM_sl6_ll1_pl3_dm512_nh8_el2_dl2_df2048_expand2_dc4_fc1_eblearned_dtTrue_test_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 19
val 1
test 4
  0%|          | 0/19 [00:00<?, ?it/s]Killed
