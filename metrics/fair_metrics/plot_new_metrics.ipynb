{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Metrics with Multiple Features\n==============================\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebook demonstrates the new API for metrics, which supports\nmultiple sensitive and conditional features. This example does not\ncontain a proper discussion of how fairness relates to the dataset used,\nalthough it does highlight issues which users may want to consider when\nanalysing their datasets.\n\nWe are going to consider a lending scenario, supposing that we have a\nmodel which predicts whether or not a particular customer will repay a\nloan. This could be used as the basis of deciding whether or not to\noffer that customer a loan. With traditional metrics, we would assess\nthe model using:\n\n-   The \\'true\\' values from the test set\n-   The model predictions from the test set\n\nOur fairness metrics compute group-based fairness statistics. To use\nthese, we also need categorical columns from the test set. For this\nexample, we will include:\n\n-   The sex of each individual (two unique values)\n-   The race of each individual (three unique values)\n-   The credit score band of each individual (three unique values)\n-   Whether the loan is considered \\'large\\' or \\'small\\'\n\nAn individual\\'s sex and race should not affect a lending decision, but\nit would be legitimate to consider an individual\\'s credit score and the\nrelative size of the loan which they desired.\n\nA real scenario will be more complicated, but this will serve to\nillustrate the use of the new metrics.\n\nGetting the Data\n================\n\n*This section may be skipped. It simply creates a dataset for\nillustrative purposes*\n\nWe will use the well-known UCI \\'Adult\\' dataset as the basis of this\ndemonstration. This is not for a lending scenario, but we will regard it\nas one for the purposes of this example. We will use the existing\n\\'race\\' and \\'sex\\' columns (trimming the former to three unique\nvalues), and manufacture credit score bands and loan sizes from other\ncolumns. We start with some uncontroversial [import]{.title-ref}\nstatements:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import functools\n\nimport numpy as np\nimport sklearn.metrics as skm\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.compose import make_column_selector as selector\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\n\nfrom fairlearn.datasets import fetch_adult\nfrom fairlearn.metrics import MetricFrame, count, selection_rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we import the data:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data = fetch_adult()\nX_raw = data.data.copy()\ny = (data.target == \">50K\") * 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For purposes of clarity, we consolidate the \\'race\\' column to have\nthree unique values:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def race_transform(input_str):\n    \"\"\"Reduce values to White, Black and Other.\"\"\"\n    result = \"Other\"\n    if input_str == \"White\" or input_str == \"Black\":\n        result = input_str\n    return result\n\n\nX_raw[\"race\"] = X_raw[\"race\"].map(race_transform).fillna(\"Other\").astype(\"category\")\nprint(np.unique(X_raw[\"race\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we manufacture the columns for the credit score band and requested\nloan size. These are wholly constructed, and not part of the actual\ndataset in any way. They are simply for illustrative purposes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def marriage_transform(m_s_string):\n    \"\"\"Perform some simple manipulations.\"\"\"\n    result = \"Low\"\n    if m_s_string.startswith(\"Married\"):\n        result = \"Medium\"\n    elif m_s_string.startswith(\"Widowed\"):\n        result = \"High\"\n    return result\n\n\ndef occupation_transform(occ_string):\n    \"\"\"Perform some simple manipulations.\"\"\"\n    result = \"Small\"\n    # The isinstance check is to guard against 'missing'\n    # data marked with NaN\n    if not isinstance(occ_string, float) and occ_string.startswith(\"Machine\"):\n        result = \"Large\"\n    return result\n\n\ncol_credit = X_raw[\"marital-status\"].map(marriage_transform).fillna(\"Low\")\ncol_credit.name = \"Credit Score\"\ncol_loan_size = X_raw[\"occupation\"].map(occupation_transform).fillna(\"Small\")\ncol_loan_size.name = \"Loan Size\"\n\nA = X_raw[[\"race\", \"sex\"]].copy()\nA[\"Credit Score\"] = col_credit\nA[\"Loan Size\"] = col_loan_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have imported our dataset and manufactured a few features,\nwe can perform some more conventional processing. To avoid the problem\nof [data\nleakage](https://en.wikipedia.org/wiki/Leakage_(machine_learning)), we\nneed to split the data into training and test sets before applying any\ntransforms or scaling:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "(X_train, X_test, y_train, y_test, A_train, A_test) = train_test_split(\n    X_raw, y, A, test_size=0.3, random_state=54321, stratify=y\n)\n\n# Ensure indices are aligned between X, y and A,\n# after all the slicing and splitting of DataFrames\n# and Series\n\nX_train = X_train.reset_index(drop=True)\nX_test = X_test.reset_index(drop=True)\ny_train = y_train.reset_index(drop=True)\ny_test = y_test.reset_index(drop=True)\nA_train = A_train.reset_index(drop=True)\nA_test = A_test.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we build two `~sklearn.pipeline.Pipeline`{.interpreted-text\nrole=\"class\"} objects to process the columns, one for numeric data, and\nthe other for categorical data. Both impute missing values; the\ndifference is whether the data are scaled (numeric columns) or one-hot\nencoded (categorical columns). Imputation of missing values should\ngenerally be done with care, since it could potentially introduce\nbiases. Of course, removing rows with missing data could also cause\ntrouble, if particular subgroups have poorer data quality.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "numeric_transformer = Pipeline(steps=[(\"impute\", SimpleImputer()), (\"scaler\", StandardScaler())])\ncategorical_transformer = Pipeline(\n    [\n        (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n        (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\")),\n    ]\n)\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"num\", numeric_transformer, selector(dtype_exclude=\"category\")),\n        (\"cat\", categorical_transformer, selector(dtype_include=\"category\")),\n    ]\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With our preprocessor defined, we can now build a new pipeline which\nincludes an Estimator:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "unmitigated_predictor = Pipeline(\n    steps=[\n        (\"preprocessor\", preprocessor),\n        (\"classifier\", LogisticRegression(solver=\"liblinear\", fit_intercept=True)),\n    ]\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the pipeline fully defined, we can first train it with the training\ndata, and then generate predictions from the test data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "unmitigated_predictor.fit(X_train, y_train)\ny_pred = unmitigated_predictor.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Analysing the Model with Metrics\n================================\n\nAfter our data manipulations and model training, we have the following\nfrom our test set:\n\n-   A vector of true values called `y_test`\n-   A vector of model predictions called `y_pred`\n-   A DataFrame of categorical features relevant to fairness called\n    `A_test`\n\nIn a traditional model analysis, we would now look at some metrics\nevaluated on the entire dataset. Suppose in this case, the relevant\nmetrics are `fairlearn.metrics.selection_rate`{.interpreted-text\nrole=\"func\"} and `sklearn.metrics.fbeta_score`{.interpreted-text\nrole=\"func\"} (with `beta=0.6`). We can evaluate these metrics directly:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"Selection Rate:\", selection_rate(y_test, y_pred))\nprint(\"fbeta:\", skm.fbeta_score(y_test, y_pred, beta=0.6))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We know that there are sensitive features in our data, and we want to\nensure that we\\'re not harming individuals due to membership in any of\nthese groups. For this purpose, Fairlearn provides the\n`fairlearn.metrics.MetricFrame`{.interpreted-text role=\"class\"} class.\nLet us construct an instance of this class, and then look at its\ncapabilities:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fbeta_06 = functools.partial(skm.fbeta_score, beta=0.6, zero_division=1)\n\nmetric_fns = {\"selection_rate\": selection_rate, \"fbeta_06\": fbeta_06, \"count\": count}\n\ngrouped_on_sex = MetricFrame(\n    metrics=metric_fns, y_true=y_test, y_pred=y_pred, sensitive_features=A_test[\"sex\"]\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `fairlearn.metrics.MetricFrame`{.interpreted-text role=\"class\"}\nobject requires a minimum of four arguments:\n\n1.  The underlying metric function(s) to be evaluated\n2.  The true values\n3.  The predicted values\n4.  The sensitive feature values\n\nThese are all passed as arguments to the constructor. If more than one\nunderlying metric is required (as in this case), then we must provide\nthem in a dictionary.\n\nThe underlying metrics must have a signature `fn(y_true, y_pred)`, so we\nhave to use `functools.partial`{.interpreted-text role=\"func\"} on\n`fbeta_score()` to furnish `beta=0.6` (we will show how to pass in extra\narray arguments such as sample weights shortly).\n\nWe will now take a closer look at the\n`fairlearn.metrics.MetricFrame`{.interpreted-text role=\"class\"} object.\nFirst, there is the `overall` property, which contains the metrics\nevaluated on the entire dataset. We see that this contains the same\nvalues calculated above:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "assert grouped_on_sex.overall[\"selection_rate\"] == selection_rate(y_test, y_pred)\nassert grouped_on_sex.overall[\"fbeta_06\"] == skm.fbeta_score(y_test, y_pred, beta=0.6)\nprint(grouped_on_sex.overall)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The other property in the\n`fairlearn.metrics.MetricFrame`{.interpreted-text role=\"class\"} object\nis `by_group`. This contains the metrics evaluated on each subgroup\ndefined by the categories in the `sensitive_features=` argument. Note\nthat `fairlearn.metrics.count`{.interpreted-text role=\"func\"} can be\nused to display the number of data points in each subgroup. In this\ncase, we have results for males and females:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "grouped_on_sex.by_group"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can immediately see a substantial disparity in the selection rate\nbetween males and females.\n\nWe can also create another\n`fairlearn.metrics.MetricFrame`{.interpreted-text role=\"class\"} object\nusing race as the sensitive feature:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "grouped_on_race = MetricFrame(\n    metrics=metric_fns, y_true=y_test, y_pred=y_pred, sensitive_features=A_test[\"race\"]\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `overall` property is unchanged:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "assert (grouped_on_sex.overall == grouped_on_race.overall).all()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `by_group` property now contains the metrics evaluated based on the\n\\'race\\' column:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "grouped_on_race.by_group"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that there is also a significant disparity in selection rates\nwhen grouping by race.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sample weights and other arrays\n===============================\n\nWe noted above that the underlying metric functions passed to the\n`fairlearn.metrics.MetricFrame`{.interpreted-text role=\"class\"}\nconstructor need to be of the form `fn(y_true, y_pred)` - we do not\nsupport scalar arguments such as `pos_label=` or `beta=` in the\nconstructor. Such arguments should be bound into a new function using\n`functools.partial`{.interpreted-text role=\"func\"}, and the result\npassed in. However, we do support arguments which have one entry for\neach sample, with an array of sample weights being the most common\nexample. These are divided into subgroups along with `y_true` and\n`y_pred`, and passed along to the underlying metric.\n\nTo use these arguments, we pass in a dictionary as the `sample_params=`\nargument of the constructor. Let us generate some random weights, and\npass these along:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "random_weights = np.random.rand(len(y_test))\n\nexample_sample_params = {\n    \"selection_rate\": {\"sample_weight\": random_weights},\n    \"fbeta_06\": {\"sample_weight\": random_weights},\n}\n\n\ngrouped_with_weights = MetricFrame(\n    metrics=metric_fns,\n    y_true=y_test,\n    y_pred=y_pred,\n    sensitive_features=A_test[\"sex\"],\n    sample_params=example_sample_params,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can inspect the overall values, and check they are as expected:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "assert grouped_with_weights.overall[\"selection_rate\"] == selection_rate(\n    y_test, y_pred, sample_weight=random_weights\n)\nassert grouped_with_weights.overall[\"fbeta_06\"] == skm.fbeta_score(\n    y_test, y_pred, beta=0.6, sample_weight=random_weights\n)\nprint(grouped_with_weights.overall)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also see the effect on the metric being evaluated on the\nsubgroups:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "grouped_with_weights.by_group"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Quantifying Disparities\n=======================\n\nWe now know that our model is selecting individuals who are female far\nless often than individuals who are male. There is a similar effect when\nexamining the results by race, with blacks being selected far less often\nthan whites (and those classified as \\'other\\'). However, there are many\ncases where presenting all these numbers at once will not be useful (for\nexample, a high level dashboard which is monitoring model performance).\nFairlearn provides several means of aggregating metrics across the\nsubgroups, so that disparities can be readily quantified.\n\nThe simplest of these aggregations is `group_min()`, which reports the\nminimum value seen for a subgroup for each underlying metric (we also\nprovide `group_max()`). This is useful if there is a mandate that \\\"no\nsubgroup should have an `fbeta_score()` of less than 0.6.\\\" We can\nevaluate the minimum values easily:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "grouped_on_race.group_min()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As noted above, the selection rates varies greatly by race and by sex.\nThis can be quantified in terms of a difference between the subgroup\nwith the highest value of the metric, and the subgroup with the lowest\nvalue. For this, we provide the method\n`difference(method='between_groups)`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "grouped_on_race.difference(method=\"between_groups\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also evaluate the difference relative to the corresponding\noverall value of the metric. In this case we take the absolute value, so\nthat the result is always positive:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "grouped_on_race.difference(method=\"to_overall\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are situations where knowing the ratios of the metrics evaluated\non the subgroups is more useful. For this we have the `ratio()` method.\nWe can take the ratios between the minimum and maximum values of each\nmetric:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "grouped_on_race.ratio(method=\"between_groups\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also compute the ratios relative to the overall value for each\nmetric. Analogous to the differences, the ratios are always in the range\n$[0,1]$:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "grouped_on_race.ratio(method=\"to_overall\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Intersections of Features\n=========================\n\nSo far we have only considered a single sensitive feature at a time, and\nwe have already found some serious issues in our example data. However,\nsometimes serious issues can be hiding in intersections of features. For\nexample, the [Gender Shades\nproject](https://www.media.mit.edu/projects/gender-shades/overview/)\nfound that facial recognition algorithms performed worse for blacks than\nwhites, and also worse for women than men (despite overall high accuracy\nscore). Moreover, performance on black females was *terrible*. We can\nexamine the intersections of sensitive features by passing multiple\ncolumns to the `fairlearn.metrics.MetricFrame`{.interpreted-text\nrole=\"class\"} constructor:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "grouped_on_race_and_sex = MetricFrame(\n    metrics=metric_fns,\n    y_true=y_test,\n    y_pred=y_pred,\n    sensitive_features=A_test[[\"race\", \"sex\"]],\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The overall values are unchanged, but the `by_group` table now shows the\nintersections between subgroups:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "assert (grouped_on_race_and_sex.overall == grouped_on_race.overall).all()\ngrouped_on_race_and_sex.by_group"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The aggregations are still performed across all subgroups for each\nmetric, so each continues to reduce to a single value. If we look at the\n`group_min()`, we see that we violate the mandate we specified for the\n`fbeta_score()` suggested above (for females with a race of \\'Other\\' in\nfact):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "grouped_on_race_and_sex.group_min()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Looking at the `ratio()` method, we see that the disparity is worse\n(specifically between white males and black females, if we check in the\n`by_group` table):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "grouped_on_race_and_sex.ratio(method=\"between_groups\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Control Features\n================\n\nThere is a further way we can slice up our data. We have (*completely\nmade up*) features for the individuals\\' credit scores (in three bands)\nand also the size of the loan requested (large or small). In our loan\nscenario, it is acceptable that individuals with high credit scores are\nselected more often than individuals with low credit scores. However,\nwithin each credit score band, we do not want a disparity between (say)\nblack females and white males. To example these cases, we have the\nconcept of *control features*.\n\nControl features are introduced by the `control_features=` argument to\nthe `fairlearn.metrics.MetricFrame`{.interpreted-text role=\"class\"}\nobject:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cond_credit_score = MetricFrame(\n    metrics=metric_fns,\n    y_true=y_test,\n    y_pred=y_pred,\n    sensitive_features=A_test[[\"race\", \"sex\"]],\n    control_features=A_test[\"Credit Score\"],\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This has an immediate effect on the `overall` property. Instead of\nhaving one value for each metric, we now have a value for each unique\nvalue of the control feature:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cond_credit_score.overall"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `by_group` property is similarly expanded:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cond_credit_score.by_group"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The aggregates are also evaluated once for each group identified by the\ncontrol feature:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cond_credit_score.group_min()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cond_credit_score.ratio(method=\"between_groups\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In our data, we see that we have a dearth of positive results for high\nincome non-whites, which significantly affects the aggregates.\n\nWe can continue adding more control features:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cond_both = MetricFrame(\n    metrics=metric_fns,\n    y_true=y_test,\n    y_pred=y_pred,\n    sensitive_features=A_test[[\"race\", \"sex\"]],\n    control_features=A_test[[\"Loan Size\", \"Credit Score\"]],\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `overall` property now splits into more values:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cond_both.overall"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As does the `by_groups` property, where `NaN` values indicate that there\nwere no samples in the cell:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cond_both.by_group"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The aggregates behave similarly. By this point, we are having\nsignificant issues with under-populated intersections. Consider:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def member_counts(y_true, y_pred):\n    assert len(y_true) == len(y_pred)\n    return len(y_true)\n\n\ncounts = MetricFrame(\n    metrics=member_counts,\n    y_true=y_test,\n    y_pred=y_pred,\n    sensitive_features=A_test[[\"race\", \"sex\"]],\n    control_features=A_test[[\"Loan Size\", \"Credit Score\"]],\n)\n\ncounts.by_group"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Recall that `NaN` indicates that there were no individuals in a cell -\n`member_counts()` will not even have been called.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Exporting from MetricFrame\n==========================\n\nSometimes, we need to extract our data for use in other tools. For this,\nwe can use the :py`pandas.DataFrame.to_csv`{.interpreted-text\nrole=\"meth\"} method, since the\n:py`~fairlearn.metrics.MetricFrame.by_group`{.interpreted-text\nrole=\"meth\"} property will be a `pandas.DataFrame`{.interpreted-text\nrole=\"class\"} (or in a few cases, it will be a\n`pandas.Series`{.interpreted-text role=\"class\"}, but that has a similar\n:py`~pandas.Series.to_csv`{.interpreted-text role=\"meth\"} method):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "csv_output = cond_credit_score.by_group.to_csv()\nprint(csv_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The :py`pandas.DataFrame.to_csv`{.interpreted-text role=\"meth\"} method\nhas a large number of arguments to control the exported CSV. For\nexample, it can write directly to a CSV file, rather than returning a\nstring (as shown above).\n\nThe `~fairlearn.metrics.MetricFrame.overall`{.interpreted-text\nrole=\"meth\"} property can be handled similarly, in the cases that it is\nnot a scalar.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}