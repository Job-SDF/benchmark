{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "MetricFrame: Beyond Binary Classification\n=========================================\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebook contains examples of using\n`~fairlearn.metrics.MetricFrame`{.interpreted-text role=\"class\"} for\ntasks which go beyond simple binary classification.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import functools\n\nimport numpy as np\nimport sklearn.metrics as skm\n\nfrom fairlearn.metrics import MetricFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Multiclass & Nonscalar Results\n==============================\n\nSuppose we have a multiclass problem, with labels $\\in {0, 1, 2}$, and\nthat we wish to generate confusion matrices for each subgroup identified\nby the sensitive feature $\\in { a, b, c, d}$. This is supported readily\nby `~fairlearn.metrics.MetricFrame`{.interpreted-text role=\"class\"},\nwhich does not require the result of a metric to be a scalar.\n\nFirst, let us generate some random input data:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "rng = np.random.default_rng(seed=96132)\n\nn_rows = 1000\nn_classes = 3\nn_sensitive_features = 4\n\ny_true = rng.integers(n_classes, size=n_rows)\ny_pred = rng.integers(n_classes, size=n_rows)\n\ntemp = rng.integers(n_sensitive_features, size=n_rows)\ns_f = [chr(ord(\"a\") + x) for x in temp]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To use `~sklearn.metrics.confusion_matrix`{.interpreted-text\nrole=\"func\"}, we need to prebind the [labels]{.title-ref} argument,\nsince it is possible that some of the subgroups will not contain all of\nthe possible labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "conf_mat = functools.partial(skm.confusion_matrix, labels=np.unique(y_true))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With this now available, we can create our\n`~fairlearn.metrics.MetricFrame`{.interpreted-text role=\"class\"}:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mf = MetricFrame(\n    metrics={\"conf_mat\": conf_mat}, y_true=y_true, y_pred=y_pred, sensitive_features=s_f\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From this, we can view the overall confusion matrix:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mf.overall"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And also the confusion matrices for each subgroup:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mf.by_group"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Obviously, the other methods such as\n`~fairlearn.metrics.MetricFrame.group_min`{.interpreted-text\nrole=\"meth\"} will not work, since operations such as \\'less than\\' are\nnot well defined for matrices.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Metric functions with different return types can also be mixed in a\nsingle `~fairlearn.metrics.MetricFrame`{.interpreted-text role=\"class\"}.\nFor example:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "recall = functools.partial(skm.recall_score, average=\"macro\")\n\nmf2 = MetricFrame(\n    metrics={\"conf_mat\": conf_mat, \"recall\": recall},\n    y_true=y_true,\n    y_pred=y_pred,\n    sensitive_features=s_f,\n)\n\nprint(\"Overall values\")\nprint(mf2.overall)\nprint(\"Values by group\")\nprint(mf2.by_group)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Non-scalar Inputs\n=================\n\n`~fairlearn.metrics.MetricFrame`{.interpreted-text role=\"class\"} does\nnot require its inputs to be scalars either. To demonstrate this, we\nwill use an image recognition example (kindly supplied by Ferdane\nBekmezci, Hamid Vaezi Joze and Samira Pouyanfar).\n\nImage recognition algorithms frequently construct a bounding box around\nregions where they have found their target features. For example, if an\nalgorithm detects a face in an image, it will place a bounding box\naround it. These bounding boxes constitute [y\\_pred]{.title-ref} for\n`~fairlearn.metrics.MetricFrame`{.interpreted-text role=\"class\"}. The\n[y\\_true]{.title-ref} values then come from bounding boxes marked by\nhuman labellers.\n\nBounding boxes are often compared using the \\'iou\\' metric. This\ncomputes the intersection and the union of the two bounding boxes, and\nreturns the ratio of their areas. If the bounding boxes are identical,\nthen the metric will be 1; if disjoint then it will be 0. A function to\ndo this is:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def bounding_box_iou(box_A_input, box_B_input):\n    # The inputs are array-likes in the form\n    # [x_0, y_0, delta_x,delta_y]\n    # where the deltas are positive\n\n    box_A = np.array(box_A_input)\n    box_B = np.array(box_B_input)\n\n    if box_A[2] < 0:\n        raise ValueError(\"Bad delta_x for box_A\")\n    if box_A[3] < 0:\n        raise ValueError(\"Bad delta y for box_A\")\n    if box_B[2] < 0:\n        raise ValueError(\"Bad delta x for box_B\")\n    if box_B[3] < 0:\n        raise ValueError(\"Bad delta y for box_B\")\n\n    # Convert deltas to coordinates\n    box_A[2:4] = box_A[0:2] + box_A[2:4]\n    box_B[2:4] = box_B[0:2] + box_B[2:4]\n\n    # Determine the (x, y)-coordinates of the intersection rectangle\n    x_A = max(box_A[0], box_B[0])\n    y_A = max(box_A[1], box_B[1])\n    x_B = min(box_A[2], box_B[2])\n    y_B = min(box_A[3], box_B[3])\n\n    if (x_B < x_A) or (y_B < y_A):\n        return 0\n\n    # Compute the area of intersection rectangle\n    interArea = (x_B - x_A) * (y_B - y_A)\n\n    # Compute the area of both the prediction and ground-truth\n    # rectangles\n    box_A_area = (box_A[2] - box_A[0]) * (box_A[3] - box_A[1])\n    box_B_area = (box_B[2] - box_B[0]) * (box_B[3] - box_B[1])\n\n    # Compute the intersection over union by taking the intersection\n    # area and dividing it by the sum of prediction + ground-truth\n    # areas - the intersection area\n    iou = interArea / float(box_A_area + box_B_area - interArea)\n\n    return iou"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is a metric for two bounding boxes, but for\n`~fairlearn.metrics.MetricFrame`{.interpreted-text role=\"class\"} we need\nto compare two lists of bounding boxes. For the sake of simplicity, we\nwill return the mean value of \\'iou\\' for the two lists, but this is by\nno means the only choice:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def mean_iou(true_boxes, predicted_boxes):\n    if len(true_boxes) != len(predicted_boxes):\n        raise ValueError(\"Array size mismatch\")\n\n    all_iou = [\n        bounding_box_iou(y_true, y_pred) for y_true, y_pred in zip(true_boxes, predicted_boxes)\n    ]\n\n    return np.mean(all_iou)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We need to generate some input data, so first create a function to\ngenerate a single random bounding box:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def generate_bounding_box(max_coord, max_delta, rng):\n    corner = max_coord * rng.random(size=2)\n    delta = max_delta * rng.random(size=2)\n\n    return np.concatenate((corner, delta))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now use this to create sample [y\\_true]{.title-ref} and\n[y\\_pred]{.title-ref} arrays of bounding boxes:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def many_bounding_boxes(n_rows, max_coord, max_delta, rng):\n    return [generate_bounding_box(max_coord, max_delta, rng) for _ in range(n_rows)]\n\n\ntrue_bounding_boxes = many_bounding_boxes(n_rows, 5, 10, rng)\npred_bounding_boxes = many_bounding_boxes(n_rows, 5, 10, rng)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we can use these in a\n`~fairlearn.metrics.MetricFrame`{.interpreted-text role=\"class\"}:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mf_bb = MetricFrame(\n    metrics={\"mean_iou\": mean_iou},\n    y_true=true_bounding_boxes,\n    y_pred=pred_bounding_boxes,\n    sensitive_features=s_f,\n)\n\nprint(\"Overall metric\")\nprint(mf_bb.overall)\nprint(\"Metrics by group\")\nprint(mf_bb.by_group)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The individual entries in the [y\\_true]{.title-ref} and\n[y\\_pred]{.title-ref} arrays can be arbitrarily complex. It is the\nmetric functions which give meaning to them. Similarly,\n`~fairlearn.metrics.MetricFrame`{.interpreted-text role=\"class\"} does\nnot impose restrictions on the return type. One can envisage an image\nrecognition task where there are multiple detectable objects in each\npicture, and the image recognition algorithm produces multiple bounding\nboxes (not necessarily in a 1-to-1 mapping either). The output of such a\nscenario might well be a matrix of some description. Another case where\nboth the input data and the metrics will be complex is natural language\nprocessing, where each row of the input could be an entire sentence,\npossibly with complex word embeddings included.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Conclusion\n==========\n\nThis notebook has given some taste of the flexibility of\n`~fairlearn.metrics.MetricFrame`{.interpreted-text role=\"class\"} when it\ncomes to inputs, outputs and metric functions. The input arrays can have\nelements of arbitrary types, and the return values from the metric\nfunctions can also be of any type (although methods such as\n`~fairlearn.metrics.MetricFrame.group_min`{.interpreted-text\nrole=\"meth\"} may not work).\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}