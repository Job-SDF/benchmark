{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plotting Metrics with Errors\n============================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.compose import make_column_selector as selector\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import accuracy_score, confusion_matrix, recall_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.utils import check_consistent_length\n\nfrom fairlearn.datasets import fetch_adult"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load and preprocess the data set\n================================\n\nWe start by importing the various modules we\\'re going to use:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from fairlearn.experimental.enable_metric_frame_plotting import plot_metric_frame\nfrom fairlearn.metrics import MetricFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We download the data set using the function\n`.fetch_adult`{.interpreted-text role=\"func\"} in\n`fairlearn.datasets`{.interpreted-text role=\"mod\"}. The original Adult\ndata set can be found at <https://archive.ics.uci.edu/ml/datasets/Adult>\nThere are some caveats to using this dataset, but we will use it solely\nas an example to demonstrate the functionality of plotting metrics with\nerror bars. We use a pipeline to preprocess the data then use a\n:py`sklearn.tree.DecisionTreeClassifier`{.interpreted-text role=\"class\"}\nto make predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data = fetch_adult()\nX = data.data\ny = (data.target == \">50K\").astype(int)\nX_train, X_test, y_train_true, y_test_true = train_test_split(\n    X, y, test_size=0.33, random_state=42\n)\n\nnumeric_transformer = Pipeline(\n    steps=[\n        (\"impute\", SimpleImputer()),\n        (\"scaler\", StandardScaler()),\n    ]\n)\n\ncategorical_transformer = Pipeline(\n    [\n        (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n        (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n    ]\n)\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"num\", numeric_transformer, selector(dtype_exclude=\"category\")),\n        (\"cat\", categorical_transformer, selector(dtype_include=\"category\")),\n    ]\n)\n\ncomplete_pipeline = Pipeline(\n    [\n        (\"preprocessor\", preprocessor),\n        (\n            \"estimator\",\n            DecisionTreeClassifier(min_samples_leaf=10, max_depth=4),\n        ),\n    ]\n)\n\ncomplete_pipeline.fit(X_train, y_train_true)\ny_test_pred = complete_pipeline.predict(X_test)\ntest_set_sex = X_test[\"race\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Confidence interval calculations\n================================\n\nWe have many different choices for calculating confidence intervals. In\nthis notebook we\\'ll just be using a [Wilson score\ninterval](https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval#Wilson_score_interval).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# We aim to create a 95% confidence interval, so we use a :code:`z_score` of 1.959964\nz_score = 1.959964\ndigits_of_precision = 4\nerror_labels_legend = \"95% Confidence Interval\"\n\n\ndef general_wilson(p, n, digits=4, z=1.959964):\n    \"\"\"Return lower and upper bound of a Wilson confidence interval.\n\n    Parameters\n    ----------\n    p : float\n        Proportion of successes.\n    n : int\n        Total number of trials.\n    digits : int\n        Digits of precision to which the returned bound will be rounded\n    z : float\n        Z-score, which indicates the number of standard deviations of confidence.\n        The default value of 1.959964 is for a 95% confidence interval\n\n    Returns\n    -------\n        np.ndarray\n        Array of length 2 of form: [lower_bound, upper_bound]\n    \"\"\"\n    denominator = 1 + z**2 / n\n    centre_adjusted_probability = p + z * z / (2 * n)\n    adjusted_standard_deviation = np.sqrt((p * (1 - p) + z * z / (4 * n))) / np.sqrt(n)\n    lower_bound = (centre_adjusted_probability - z * adjusted_standard_deviation) / denominator\n    upper_bound = (centre_adjusted_probability + z * adjusted_standard_deviation) / denominator\n    return np.array([round(lower_bound, digits), round(upper_bound, digits)])\n\n\ndef recall_wilson(y_true, y_pred):\n    \"\"\"Return a Wilson confidence interval for the recall metric.\n\n    Parameters\n    ----------\n    y_true : array-like of shape (n_samples,)\n        Ground truth labels\n    y_pred : array-like of shape (n_samples,)\n        Predicted labels\n\n    Returns\n    -------\n        np.ndarray\n        Array of length 2 of form: [lower_bound, upper_bound]\n    \"\"\"\n    check_consistent_length(y_true, y_pred)\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n    bounds = general_wilson(tp / (tp + fn), tp + fn, digits_of_precision, z_score)\n    return bounds\n\n\ndef accuracy_wilson(y_true, y_pred):\n    \"\"\"Return a Wilson confidence interval for the accuracy metric.\n\n    Parameters\n    ----------\n    y_true : array-like of shape (n_samples,)\n        Ground truth labels\n    y_pred : array-like of shape (n_samples,)\n        Predicted labels\n\n    Returns\n    -------\n        np.ndarray\n        Array of length 2 of form: [lower_bound, upper_bound]\n    \"\"\"\n    check_consistent_length(y_true, y_pred)\n    score = accuracy_score(y_true, y_pred)\n    bounds = general_wilson(score, len(y_true), digits_of_precision, z_score)\n    return bounds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "MetricFrame\n===========\n\nNow we create a `fairlearn.metrics.MetricFrame`{.interpreted-text\nrole=\"class\"} to generate the Wilson bounds for accuracy and recall\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Analyze metrics using :class:`fairlearn.metrics.MetricFrame`\nmetrics_dict = {\n    \"Recall\": recall_score,\n    \"Recall Bounds\": recall_wilson,\n    \"Accuracy\": accuracy_score,\n    \"Accuracy Bounds\": accuracy_wilson,\n}\nmetric_frame = MetricFrame(\n    metrics=metrics_dict,\n    y_true=y_test_true,\n    y_pred=y_test_pred,\n    sensitive_features=test_set_sex,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plotting\n========\n\nPlot metrics without confidence intervals\n-----------------------------------------\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_metric_frame(metric_frame, kind=\"point\", metrics=[\"Recall\", \"Accuracy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot metrics with confidence intervals (possibly asymmetric)\n============================================================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_metric_frame(\n    metric_frame,\n    kind=\"bar\",\n    metrics=[\"Recall\", \"Accuracy\"],\n    conf_intervals=[\"Recall Bounds\", \"Accuracy Bounds\"],\n    plot_ci_labels=True,\n    subplots=False,\n)\nplot_metric_frame(\n    metric_frame,\n    kind=\"point\",\n    metrics=\"Recall\",\n    conf_intervals=\"Recall Bounds\",\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot metrics with error labels\n==============================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_metric_frame(\n    metric_frame,\n    kind=\"bar\",\n    metrics=\"Recall\",\n    conf_intervals=\"Recall Bounds\",\n    colormap=\"Pastel1\",\n    plot_ci_labels=True,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plots all columns and treats them as metrics without error bars\n===============================================================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_metric_frame(metric_frame, kind=\"bar\", colormap=\"rainbow\", layout=[1, 2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Customizing plots\n=================\n\n`.plot_metric_frame`{.interpreted-text role=\"func\"} returns an\n`matplotlib.axes.Axes`{.interpreted-text role=\"class\"} object that we\ncan customize further.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "axs = plot_metric_frame(\n    metric_frame,\n    kind=\"point\",\n    metrics=[\"Recall\", \"Accuracy\"],\n    conf_intervals=[\"Recall Bounds\", \"Accuracy Bounds\"],\n    subplots=True,\n    ci_labels_legend=error_labels_legend,\n    # the following parameters are passed into `pandas.DataFrame.plot` as kwargs\n    layout=[1, 2],\n    rot=45,\n    colormap=\"rainbow\",\n    figsize=(12, 4),\n)\naxs[0][0].set_ylabel(\"Recall\")\naxs[0][0].set_title(\"Recall Plot\")\naxs[0][1].set_title(\"Accuracy Plot\")\naxs[0][0].set_xlabel(\"Race\")\naxs[0][1].set_xlabel(\"Race\")\naxs[0][0].set_ylabel(\"Recall\")\naxs[0][1].set_ylabel(\"Accuracy\")\n\n# Set the y-scale for both metrics to [0, 1]\naxs[0][0].set_ylim((0, 1))\naxs[0][1].set_ylim((0, 1))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}