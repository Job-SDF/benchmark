{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Intersectionality in Mental Health Care\n=======================================\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*This notebook was written by Yifan Wang, Marta Maslej, and Laura\nSikstrom and is licenced under a* [Creative Commons Attribution 4.0\nInternational License](http://creativecommons.org/licenses/by/4.0/).\n\n------------------------------------------------------------------------\n\nThere is an increasing interest in applying innovations in artificial\nintelligence to provide more efficient, precise, or personalized patient\ncare. Specifically, the ability of machine learning (ML) to identify\npatterns in large and complex datasets holds tremendous promise for\nsolving some of the most complex and intractable problems in health\ncare. However, there are ongoing questions\n`ghassemi2022medicine`{.interpreted-text role=\"footcite\"} about a range\nof known **gendered and racialized biases** - arising from diagnostic\ntools, clinical interactions, and health policies -that get *baked* into\nthese datasets. In nephrology, for example, algorithms developed to\nestimate glomerular filtration rate assign higher values (which suggest\nbetter kidney function) to Black individuals\n`vyas2020hidden`{.interpreted-text role=\"footcite\"}, which could delay\ntreatent for this patient group while ultimately worsening outcomes. In\nturn, any ML algorithms that are trained on this data could exhibit\n**differences in predictive performance across certain groups** - not by\nany flaw of the algorithm itself, but because it is capturing societal\nbiases that are encoded into the data itself. Upon deployment, these\ntools can *amplify harms* for marginalized populations, particularly\nthose defined by intersections of features (e.g., gender, race, class).\n\nIn this tutorial, we will examine another instance of this: in\npsychiatric diagnosis data, Black men are much more likely to be\nmisdiagnosed with schizophrenia as compared to white men due to factors\nsuch as diagnostic bias by clinicians. Through this case study, we\ndemonstrate the **value of applying an interdisciplinary approach to\nanalyzing intersectional biases**, towards ensuring that these\ninnovative tools are implemented safely and ethically.\n\n------------------------------------------------------------------------\n\n**Learning objectives**. This notebook has three main learning\nobjectives. After this tutorial, we hope that you will be able to:\n\n1.  Think critically about how populations can be defined and how this\n    relates to the measurement, identification, and interpretation of\n    health inequities\n2.  Explain the advantages of an intersectional approach to research\n3.  Conduct an intersectional bias assessment of simulated psychiatric\n    data, based on a finding from the psychiatric literature\n\n------------------------------------------------------------------------\n\nWhat is a fair machine learning model?\n======================================\n\nSikstrom et al (2022) identify 3 general pillars of fairness as it\npertains to ML: Transparency, Inclusion, and Impartiality\n`sikstrom2022conceptualising`{.interpreted-text role=\"footcite\"}\n\n**Transparency**: A range of methods designed to see, understand, and\nhold complex algorithmic systems accountable in a timely fashion.\n\n**Inclusion**: The process of improving the ability, opportunity, and\ndignity of people, disadvantaged on the basis of their identity, to\naccess health services, receive compassionate care, and achieve\nequitable treatment outcomes.\n\n**Impartiality**: Health care should be free from unfair bias and\nsystemic discrimination. Deploying a ML algorithm requires a\nsociotechnical understanding of how data is collected and interpreted\nwithin algorithmic system in order to ensure its responsible\nimplementation in a clinical setting.\n\nAlthough all aspects of fairness are equally important, this tutorial\nwill focus on impartiality. Specifically, we aim to examine fairness\nthrough the scope of intersectionality, which was originally coined by\nKimberle Crenshaw `crenshaw1991intersectionality`{.interpreted-text\nrole=\"footcite\"}:\n\n> **Intersectionality** is a framework for understanding how different\n> forms of inequality (e.g., gender and race) often operate together and\n> exacerbate each other.\n\nWhile by no means exhaustive, this serves as a useful frame that can be\ncoupled with other fairness approaches to enable a more thoughtful\ndiscussion of fairness-related issues.\n\nWho and what is a population?\n=============================\n\nWhen we do a fairness assessment, we need to decide which groups of\npeople to compare to identify whether there is some kind of\nfairness-related harm occurring.\n\nThe key question in being able to do so is this: **Who and what is a\npopulation?** It may seem like this question is trivial, and has a\nclear-cut meaning in no need of further clarification. However,\nresearchers like Nancy Krieger have pointed out that a clear notion of\n\\\"population\\\" is rarely defined\n`krieger2012population`{.interpreted-text role=\"footcite\"}, despite its\ncentrality to fields like ML fairness.\n\nAs such, seeking a clearer answer to this question is central to ML\nbecause it enables us to determine how and when can populations be\n**meaningfully and appropriately compared**, and allows a recognition of\nwhen such comparisons may be meaningless, or even worse, misleading.\n\nA reflective activity\n---------------------\n\n*\\\"Every man is in certain respects, like all other men, like some other\nmen, like no other men.\\\"* \\-- Murray and Klukholne (1953).\n\n> Consider a group or community that you\\'ve been part of. This could be\n> anything from group of friends or colleagues, to the people you\\'re\n> currently sitting next to. Consider the following questions:\n>\n> -   *What is something that your whole group has in common?* For\n>     example, on a soccer team, this might be the fact that everyone\n>     plays soccer.\n> -   *What is something that some of your group has in common?* Going\n>     off the same soccer team example, perhaps some of the team\n>     identifies as boys, while some of the team identifies as girls.\n> -   *What is something that makes each of you unique?* Perhaps\n>     everyone is a different height, or everyone grew up in a different\n>     neighborhood.\n\nFirst, notice the intersectional approach that we took with this\nactivity - each individual is not defined by features about themselves\nin isolation, but the intersection of many different identities, which\nconstitutes the experience of that individual.\n\nSecond, note that disparities can result from both the factors that make\nus the same, and the factors that make us different. We need to keep\nthis idea in mind - when we\\'re comparing groups, are these meaningful\ndifferences that we\\'re comparing, and how do we know? For example, a\npotential similarity between a group of people is that \\\"all of us wear\nglasses\\\" - but **does this constitute a meaningful way to compare\ngroups of people?** Answering this question requires context, and can\nhelp us identify biases in our judgement that can lead to fairness\nharms, and think about possible solutions.\n\nLet\\'s look at a real-life example to see how exactly this is important.\n\nIn 2011, a paper studying the epidemiology of cancer noted that \\\"early\nonset ER negative tumors also develop more frequently in Asian Indian\nand Pakistani women and in women from other parts of Asia, although not\nas prevalent as it is in West Africa.\\\"\n`wallace2011interactions`{.interpreted-text role=\"footcite\"}\n\nAt first glance, this seems like a reasonable comparison that helps\nestablish the basis for certain populations having a higher prevalence\nof cancer.\n\nHowever, Krieger points out that the cancer incidence rates used to\narrive at this conclusion are based on:\n\n-   For Pakistan, the weighted average of observed rates within a single\n    region\n-   For India, a complex estimation involving several rates in different\n    states\n-   For West Africa, the weighted average for 16 countries:\n    -   10 of these countries have rates estimated based on neighboring\n        countries\n    -   5 rely on extrapolation from a single city within that country\n    -   Only one has a national cancer registry\n\nThis added context makes it clear that these population comparisons are\nnot so clear-cut, and that perhaps there is more nuance we need to be\nmindful of than we first thought.\n\nDefining a population\n---------------------\n\nHow then, should we conceptualize populations to enable the nuanced\nunderstanding required when we compare them? To give some background on\nthis work, we will again draw on some of the work of Nancy Krieger, an\nepidemiologist who has written extensively on the concept of\npopulations.\n\n### Populations as statistical entities\n\nMuch of Krieger\\'s work stands in contrast to the conventional\ndefinition of population, which is limited to an understanding of\npopulations as statistical objects, rather than of substantive beings.\nThis definition is as follows:\n\n> **Statistical view on populations.** Populations are (statistical)\n> entities composed of component parts defined by **innate (or\n> intrinsic)** attributes\\'\n\nImplicit in this definition is a notion of causality: if populations\ndiffered in their means, this indicated that there was either a\ndifference in \\\"essence, or in or external factors, that caused this\ndifference between populations. To this end, **populations can be\ncompared on the basis of their means because they are caused by\ncomparable differences.**\n\nThis idea that humans have **innate and comparable** attributes was\nlargely derived from Adolphe Quetelet\\'s invention of the **\\\"Average\nMan,\\\"** `eknoyan2007adolphe`{.interpreted-text role=\"footcite\"}\nestablishing the notion of a population mean.\n\nOriginally a physicist, Quetelet borrowed the idea from astronomy, where\nthe \\\"true location\\\" of a single star was determined the observations\ndone by multiple observatories. Applied to populations, this meant that\nobserving the characteristics of multiple/all the individuals within a\ngroup allowed the establishment of a \\\"true mean\\\" like human height, or\nbody weight (which is how we got the Body Mass Index).\n\nWhile the population mean of a star is a descriptor of its position in\nspace, the population mean of a human population depends on how that\npopulation is defined. For example, recall our reflective activity:\nthose similarities and differences constitute how one might define a\ngroup. This raises some interesting questions and issues.\n\n### The issues\n\n> **\\\"For a start, the location of the mean referred to the location of\n> a singular real object, whereas for a population, the location of a\n> population mean depended on how the population was defined.\\\"**\n\nWe could define a population as all those who are human, or perhaps all\nthose who are of a certain nationality, or many other possibilities.\nThinking back about the reflective activity, the elements that your\ngroup had in common and those that were different are all elements that\ncould help define a population.\n\nCrucially, we need to be careful about how we define a population,\nbecause it can impact the results we get from any analyses - perhaps it\nmight skew them, or perhaps the lack of appropriate comparison groups\nmight render an analysis inappropriate. Let\\'s look an example of this\nfrom a 2016 BBC article `bbc2016dutch`{.interpreted-text\nrole=\"footcite\"} which compares body heights:\n\n![A visualization that shows the average height of different countries in 1914 and 2014.\nThe Netherlands has the tallest average body height for men, Iran the biggest height gain\nfor men, South Korea the biggest height gain for women, and Guatemala the smallest average\nbody height for women.](../_static/images/imhc_bodyheight.png){width=\"600px\"}\n\nOn the basis of what population definitions are these comparisons being\nmade? In this case, comparisons are being made between genders,\nnationalities, and time frames.\n\nBut consider this - what exactly makes it meaningful to compare 1914 to\n2014? How can we truly interpret this data? We don\\'t have a frame of\nreference for how each population is defined. We don\\'t know their ages,\nwe don\\'t how the data was collected, and we don\\'t know why nationality\nwas considered an appropriate characteristic for comparison. These\nelements are **critical in establishing the context for whether\nsomething can be considered a meaningful comparison**, and in their\nabsence, we are left questioning.\n\nThis article is hardly unique in this problem - body mass index (BMI)\nhas become a ubiquitous way of defining obesity, but comparing\nindividuals on the basis of their BMI is problematic: the scale was\nbuilt by and for white populations, leading to an overestimation of\nhealth risks for Black individuals\n`endocrine2009widely`{.interpreted-text role=\"footcite\"}, and an\nunderestimation of health risks for Asian individuals\n`racette2003obesity`{.interpreted-text role=\"footcite\"}. Even more\ninterestingly, BMI was never meant to be a way of measuring health at an\nindividual level. `karasu2016adolphe`{.interpreted-text role=\"footcite\"}\n\nThe point is this: **social relations, not just individual traits, shape\npopulation distributions of health.**\n\nWhat this means is that we cannot exclude the context that social\nrelations *between* populations illuminate. This could include anything\nfrom gender dynamics, to class disparities, to differing economic\nsystems - all of which can have differing impacts on human health.\n\nThis point is especially important because of how **heterogeneous humans\nare**, across space and time, both between groups and within groups.\nSomeone who has written extensively about this in relation to ML,\ndisability, and design is Jutta Treviranus.\n\nTreviranus points out that the further you get from a population mean,\nthe more heterogeneous the population gets. And as you move away from\nthis \\\"average mean\\\" - not only do people get more diverse from one\nanother, but solutions that are designed with the population mean in\nmind are increasingly ineffective.\n`treviranus2019inclusivedesign`{.interpreted-text role=\"footcite\"}\n\n![A visualization that shows scattered dots.\nThe further you move away from the middle, the more scattered the points are.\nAround the points in the center, a blue circle is drawn that points to \\'design works\\'.\nMoving further from the center, a second yellow circle points to \\'design is difficult\nto use\\'. Moving even further from the center, a third orange circle points to \\'can\\'t\nuse design\\'.](../_static/images/imhc_design.png){width=\"400px\"}\n\nNotice the parallel we are drawing throughout here - we must always be\nthinking about how we compare people, and how we can make those\ncomparisons meaningful.\n\n### Proposition 2\n\nGiven these limitations, let\\'s consider an alternative understanding of\na population that :footcite`krieger2012population`{.interpreted-text\nrole=\"ct\"} proposes.\n\n> Populations are dynamic beings constituted by intrinsic relationships\n> both among their members and with other populations that together\n> produce their existence and make causal inference possible.\n\nWhat Krieger points out here, is that to make conclusions about\ndifferences between populations, we need to understand **how these\npopulations came to be, and how we define them.**\n\nKrieger\\'s view emphasizes that **identity is not a fixed attribute\n(like a star), but rather a fluid social process**. A great example that\nillustrates the degree to which identity can change can be seen in the\nracial identity of Puerto Ricans in the aftermath of Hurricane Maria.\n`godreau2021nonsovereign`{.interpreted-text role=\"footcite\"} Following\nthe hurricane, there was a 72% drop in the amount of Puerto Ricans\nidentifying as white. Did all the white people move away? Nope - they\njust no longer thought of themselves as white. The significant shift can\nbe largely attributed to Puerto Ricans feeling as though they had been\nneglected by the US government in their inadequate response to the\nhurricane, and as such, redefined themselves as Latinx. That\\'s an\nentire population collectively deciding to change their racial identity,\nand gives an example of just how dynamic - and unpredictable - identity\ncan be.\n\nThis definition of populations also recognizes that this social process\nis **linked to a history of racism and sexism**. For instance, people\\'s\nskin color was taken as being intrinsically related to specific\ncharacteristics - a stereotype which has promoted all sorts of\ninequalities in today\\'s society.\n\nA notion of populations that emphasizes their dynamic nature is helpful\nbecause it helps understand the relationships between these different\ngroups, like gender, class, and religion -all social constructions that\nhave meaning for us. It acknowledges that **we\\'re all working within\nthese systems through various different lenses, and within different\ndynamics of power and privilege**. The graphic below is a great\nrepresentation of that - of how varied all of our different experiences\ncan be:\n\n> *Intersectionality is a lens through which you can see where power\n> comes and collides, where it locks and intersects. It is the\n> acknowledgement that everyone has their own unique experiences of\n> discrimination and privilege.* \\--Kimberle Crenshaw\n\n![In the graphic above, Sylvia Duckworth uses a Spirograph to illustrate the multitude of\nways that social identities might intersect. The Spirograph is split into 12 overlapping\ncircles, each numbered, connected to a specific social identity, and assigned a unique\ncolour. To illustrate the intersections of the different social identities, where each\ncircle intersects, a new shade of the original colour is visible (as would happen when\nmixing paint colours together). At a glance the graphic shows all colours of the rainbow\nin different shades.\nThe 12 social identities listed are: race, ethnicity, gender identity,\nclass, language, religion, ability, sexuality, mental health, age, education,\nand body size.\nA quote from Kimberl\u00e9 Crenshaw appears beneath the spirograph that reads\n\"Intersectionality\nis a lens through which you can see where power comes and collides, where it locks and\nintersects.\nIt is the acknowledgement that everyone has their own unique experiences of\ndiscrimination and privilege.\" \\\"Intersectionality\\\" by Sylvia Duckworth, licensed under a\n\\`CC-BY-NC-ND\\`\\<https://creativecommons.org/licenses/by-nc-nd/2.0/\\> license.](../_static/images/imhc_intersectionality.jpg){width=\"600px\"}\n\nCritical race theory\n--------------------\n\n### Ain\\'t I A Woman?\n\nNote that although this intersectional approach has only recently been\napplied to ML, it is not a novel concept. Rather, it's a\nwell-established idea that stretches back nearly 200 years to the Black\nfeminist and the suffragette movement, when white women were fighting\nfor the right to vote, but Black women were left out of this call for\nreform. There were a range of Black activists and scholars who responded\nto this - wondering \\\"What about me? Don\\'t I matter?\\\"\n\nFor example, let us consider a quote from Sojourner Truth - an enslaved\nperson at the time. `npsSojournerTruth`{.interpreted-text\nrole=\"footcite\"}\n\n> That man over there says that women need to be helped into carriages,\n> and lifted over ditches, and to have the best place everywhere. Nobody\n> ever helps me into carriages, or over mud-puddles, or gives me any\n> best place! And ain\\'t I a woman? Look at me! Look at my arm! I have\n> ploughed and planted, and gathered into barns, and no man could head\n> me! And ain\\'t I a woman? I could work as much and eat as much as a\n> man - when I could get it - and bear the lash as well! And ain\\'t I a\n> woman? I have borne thirteen children, and seen most all sold off to\n> slavery, and when I cried out with my mother\\'s grief, none but Jesus\n> heard me! And ain\\'t I a woman?\n\nWhat she\\'s saying is \\\"I am a woman - yet all these social factors,\nlike slavery, have somehow disenfranchised me from this feminist\nmovement that\\'s going on.\\\" In asking \\\"Aint I A Woman?\\\", Sojourner\nTruth is really asking the question that\\'s been central to our\ndiscussion - what makes a population? Defining a population a specific\nway, like ignoring the realities of Black women, can cause a lot of\nharm.\n\n### Critical race theory\n\n> **Critical race theory** is an iterative methodology that draws on the\n> collective wisdom of activists and scholars to study and transform the\n> relationship between race, racism, and power. \\--\n> :footcite`ford2010critical`{.interpreted-text role=\"ct\"}\n\nIntersectionality is one tool within the critical race theory toolkit,\namong many others, and there has already been some great work\n`hanna2020towards`{.interpreted-text role=\"footcite\"} done on how\ncritical race theory (CRT) may be applied to algorithmic fairness, such\nas the need for disaggregated analysis that operates on a descriptive\nlevel in order to interrogate the most salient aspects of race for an\nalgorithmic system. A central element of CRT, especially as it relates\nto ML systems, echoes a theme we have already discussed extensively:\n**how we define a group of people matters.** The power to define a group\nof people matters, and we need to question our assumptions about what\nmakes people have something in common, and how that might affect our\nability to compare populations.\n\nThe practical applications to machine learning\n----------------------------------------------\n\nIn practice, the focus on these intersectional comparisons within the ML\nfield has been on protected groups\n`justiceCanadianHuman`{.interpreted-text role=\"footcite\"}, such as:\n\n-   Race, national or ethnic origin, color\n-   Religion\n-   Age\n-   Sex\n-   Sexual orientation\n-   Gender identity or expression\n-   Family status\n-   Marital status\n-   Disability, genetic characteristics\n\nWe have legal obligations to ensure that any medical interventions do\nnot discriminate against individuals on the basis of these attributes,\nso it makes sense that most of the ML fairness assessment have taken\nplace on race, sex or gender.\n\nHowever, it\\'s important to note that protected groups vary\nsubstantially between countries, and even across specific applications\nwithin a country. Within the United States, for example, the Fair\nHousing Act recognizes disability and gender identity as protected\nclasses while the Equal Credit Opportunity Act does not.\n\nFurthermore, the social determinants of health that are known to impact\nhealth inequalities, such as class, language, citizenship status (e.g.,\nundocumented), and geography (e.g., rural vs urban) are not protected\ngroups. How these issues are understood and measured in the literature\nare quite varied. This means that we understand *some* ways that ML can\nlead to fairness harms, but there are **many fairness harms happening\nthat we know almost nothing about.**\n\nCase Study: Mental Health Care\n==============================\n\nWe now turn to a case study on a hypothetical scenario, where we train a\nmachine learning model to predict a patient\\'s diagnosis in a mental\nhealth care setting.\n\n::: {.note}\n::: {.title}\nNote\n:::\n\nThe scenario considered in this case study is **overly simplified** and\nbased on a **simulated data set**. But we hope it gives you a sense of\nhow the application of ML can exacerbate biases in training data, and\nhow we can evaluate models for bias from an intersectional perspective.\n:::\n\nScenario\n--------\n\nTo highlight the way that fairness harms has been evaluated in health\ncare, we will first examine a study by\n:footcite`obermeyer2019dissecting`{.interpreted-text role=\"ct\"}.\n\n### Algorithmic Fairness in Health Care\n\n:footcite`obermeyer2019dissecting`{.interpreted-text role=\"ct\"} examined\na risk assessment algorithm for health insurance across approximately\n**50000 primary care patients**. This algorithm was used to identify\nindividuals who required further follow-up for complex care.\n\nThe authors found that this algorithm was **underestimating disease\nseverity in Black clients.** Why might this be the case?\n\nThey proposed several issues:\n\nFirst, the algorithm used **total health expenditures as a proxy for\ndisease severity.** This is problematic because health expenditures\nmight vary based on socioeconomic status, and poverty levels in Black\npopulations tend to be higher. As such, even if someone is very sick,\nthey may be unwilling or unable to spend money on health care. There may\nalso be an issue of trust involved, but in short, the authors didn\\'t\ntruly understand what was going on.\n\nOne of the things they point out, however, is that the bias which they\nencountered - termed \\\"Labelling Bias\\\" - is pernicious, because labels\nare measured with inequities built into them. In other words, when the\nlabels on a dataset carry structural inequities, those biases are\nunknowingly built in (we will examine this further in the applied\ncomponent of this tutorial).\n\nThe authors also note that although the developers of the algorithm\nassumed that Black and white people had some meaningful difference, they\n**didn\\'t distinguish between the two groups** based on income or gender\n- this likely led to an underestimation of how poorly this model might\nbe estimating risk for certain groups of people.\n\nA similar issue is presented by Buolamwini and Gebru in the gender\nshades project, which analyzed fairness issues in facial detection\nsoftware and found that the AI system led to larger errors for\ndark-skinned women than for other groups.\n`buolamwini2018gender`{.interpreted-text role=\"footcite\"}\n\nWith the problems that Obermeyer and colleagues investigated related to\nusing proxies as labels and keeping intersectionality in mind, let's\nmove forward with an applied problem of our own.\n\n### Background: Schizophrenia\n\nOur case scenario for this practice component of the tutorial is based\non the finding that **Black patients (and men in particular) are\ndiagnosed with schizophrenia at a higher rate than other demographic\ngroups** (for example, white men). `olbert2018meta`{.interpreted-text\nrole=\"footcite\"}\n\n> **Schizophrenia is a severe, chronic, and debilitating illness**\n> characterized by various symptoms, which are broadly divided into\n> positive and negative symptoms. Positive symptoms include the symptom\n> types that generally come to mind when thinking about schizophrenia,\n> like delusions, hallucinations, disorganized speech and thought, but\n> negative symptoms are also common. These include a lack of emotion,\n> motivation, or interest.\n\nIt's unclear why Black patients are more likely to be diagnosed with\nthis disorder, but it's likely that different factors play a role, such\nas genetics or being more likely to reside in stressful environments\n(which can be a risk factor for developing schizophrenia). However,\nthere is also some pretty compelling evidence\n`gara2019naturalistic`{.interpreted-text role=\"footcite\"} that this\neffect is **due to diagnostic bias**, or clinicians misdiagnosing black\npatients with schizophrenia when they have another illness, like an\naffective disorder or depression instead. Clinicians may be\nunderemphasizing symptoms of depression in black patients and\noveremphasizing psychotic symptoms, leading to misdiagnosis (and higher\nrates of schizophrenia). This tendency may be **particularly pronounced\nfor Black men.**\n\nWhy is this important?\n\n**Misdiagnosis can have negative downstream effects**\n`gara2012influence`{.interpreted-text role=\"footcite\"}, leading to\ninequities in care. Schizophrenia is a more serious and stigmatized\nillness than affective disorder, it has a poorer prognosis, and involves\ntreatments with greater side effects. Misdiagnosis can delay getting the\nright treatment, increase patient frustration and distrust, and worsen\nillness, all of which may be disproportionately affecting one subgroup\nof the population defined by at least two intersecting features that we\nknow of (gender and race).\n\n### The hypothetical scenario\n\nBased on this finding, we've developed a hypothetical scenario. You\nshould note that this scenario (and the simulated data we're using) is\n**overly simplified**. But we hope it gives you a sense of how the\napplication of ML can exacerbate biases in training data, and how we can\nevaluate models for bias from an intersectional perspective.\n\nImagine we have some **electronic health record data on 10,000\npatients** who have been diagnosed at *Health System A* with either\naffective disorder (denoted with 0) or schizophrenia (denoted with 1)\nover the past 10 years. This data contains information on their sex,\nrace, some psychosocial information, and information from clinical\nassessments on their symptoms. Hospital executives have had a classifier\nbuilt that will take this information collected about a patient at\nintake, and **assign a diagnosis of either affective disorder (0) or\nschizophrenia (1)**. They train a binary classifier on the data to\nassign new incoming patients a diagnosis, to triage them into the\nappropriate clinic for treatment.\n\nNow, let's say this hospital is run by some executives who would really\nlike to cut costs. They don't want to do any further assessment after\nclassification, and they are **planning to administer treatments to\npatients following this automated triage.**\n\nBut first, hospital executives must provide must provide some evidence\nto stakeholders that this classifier works well in diagnosing patients,\nso they ask the data science team to collect a test sample of 1000\npatients, on which they must evaluate the model. The executives argue\nthat the classifier works very well, based on some impressive\nsensitivity and specificity values. The stakeholders are not fully\nconvinced (thinking that the executives may be a little too eager to get\nthis model deployed), so they hire us (an independent consulting firm)\nto evaluate the model.\n\nNote that this is one overly simplistic component of our case scenario.\nTypically, much more consideration and evaluation would occur before ML\nis deployed in a real-life setting, especially in healthcare. When it\ncomes to complicated diagnoses (like schizophrenia), any use of ML is\nalso likely to complement human or clinical judgment.\n\nModel Development\n-----------------\n\nLet\\'s build our first machine learning model. First, we need to import\nthe required libraries and data set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n\n# Import relevant libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport sklearn.metrics as skm\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n\nfrom fairlearn.metrics import MetricFrame, false_positive_rate\n\n# Read in dataset\ndata_openml = fetch_openml(data_id=45040)\ndata = data_openml.data\ndata[\"Diagnosis\"] = data_openml.target\n\n# Partition the data into train and test sets\ntrain = data.loc[data.dataset == \"train\"]\ntest = data.loc[data.dataset == \"test\"]\n\ntrain = train.drop(\"dataset\", axis=1)\ntest = test.drop(\"dataset\", axis=1)\n\n\n# This function formats the data for stacked bar graphs\ndef grouppivot(labelgroup, yvalue, dataset, SZonly=False):\n    # Select only columns with a SZ diagnosis\n    if SZonly:\n        dataset = dataset.loc[dataset.Diagnosis == 0]\n\n    # Group by label group, and normalize by y value within those groups\n    grouped = (\n        dataset.groupby([labelgroup])[yvalue]\n        .value_counts(normalize=True)\n        .rename(\"percentage\")\n        .reset_index()\n    )\n\n    pivot = pd.pivot_table(\n        grouped, index=labelgroup, columns=yvalue, values=\"percentage\", aggfunc=\"sum\"\n    )\n    return pivot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Exploring the data\n==================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By examining the first few cases, we can broadly get a sense of what the\ndata looks like:\n\n-   Diagnosis is binary, with 1 corresponding to schizophrenia, and 0\n    corresponding to affective disorder.\n-   We have a binary sex variable, along with a race variable with\n    Black, Asian, White, and Hispanic as possible values. While\n    including these in a model seems problematic, we will explore the\n    problems that arise when they are removed, while also using these\n    features to conduct an intersectional bias assessment.\n-   Finally, we have a range of psychosocial and clinical variables that\n    will help the model to make a prediction\n\nYou\\'ll notice that this dataset is very clean, with no missing or\nunexpected values. If we used real-world data from hospital records, it\nwould be much messier, including:\n\n-   many missing values, and likely not missing at random (e.g.,\n    distress, impairment, or language barriers preventing a patient from\n    being able to answer questions, no resources, or staff available to\n    help, unwillingness of patients to disclose sensitive information,\n    such as disability or sexual orientation)\n-   many unexpected values, potentially due to human or system logging\n    errors (e.g., a numeric responses for a categorical variable, a\n    negative value for a variable that must be positive, such as a wait\n    time)\n-   variables that are much more complex (e.g., race or ethnicity coded\n    in a multitude of ways, `maslej2022race`{.interpreted-text\n    role=\"footcite\"} with many individuals having mixed racial\n    backgrounds, psychosocial variables never cleanly separate into\n    *stable* or *unstable* housing or *yes* or *no* for delay (in\n    reality, these constructs are rarely even captured, and they may be\n    inferred, e.g., based on location or income)\n-   subsets of older data not digitally captured\n\nReal world data for this type of task would include many more variables\nand require months of processing and linking, but we are going to use\nthis **simulated** dataset in order to convey the important ideas.\n\nKeeping this caveat in mind, let\\'s plot some graphs to better visualize\nthe data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Format graphs\ndiagnosis = grouppivot(\"dataset\", \"Diagnosis\", data)\nsex = grouppivot(\"dataset\", \"Sex\", data)\nrace = grouppivot(\"dataset\", \"Race\", data)\n\nfig, axs = plt.subplots(1, 3, figsize=(20, 4))\ndiagnosis[[1, 0]].plot.bar(stacked=True, ax=axs[0])\n\naxs[0].set_title(\"Diagnosis across train and test sets\")\nsex.plot.bar(stacked=True, ax=axs[1])\n\naxs[1].set_title(\"Sex across train and test sets\")\nrace.plot.bar(stacked=True, ax=axs[2])\n\naxs[2].set_title(\"Race across train and test sets\")\nfig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These plots show that the train and test sets have similar proportions\nof data across diagnosis, sex, and race, so the way our data is\npartitioned seems fine.\n\nWe observe a substantially higher proportion of white and Black\nindividuals compared to Asian and Hispanic individuals. If this were\nreal-world data, we might hypothesize about why this would be the case.\nIn what ways could these trends be related to systemic factors, like the\nunderrepresentation of some groups in data collection? However, this is\na dataset simulated for our case scenario, and these trends may not\nappear in real-world health systems.\n\nLet\\'s move forward with building our predictive model.\n\nPreprocessing\n=============\n\nFirst, we distinguish our outcome or label (diagnosis) from the training\nfeatures.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Split the data into x (features) and y (diagnosis)\ntrainx = train.drop(\"Diagnosis\", axis=1)\ntrainy = train.Diagnosis\n\ntestx = test.drop(\"Diagnosis\", axis=1)\ntesty = test.Diagnosis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we do some minimal preprocessing to one hot encode the categorical\nvariables, bearing in mind that preprocessing real-world hospital record\ndata is typically much more laborious.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Perform one hot encoding\ncategories = [\"Sex\", \"Race\", \"Housing\", \"Delay\"]  # Categorial variables\n\n\n# Define a function for one hot encoding\ndef onehot(data, categories=categories):\n    ordinalencoder = OneHotEncoder()\n    onehot = ordinalencoder.fit_transform(data[categories])\n\n    columns = []\n    for i, values in enumerate(ordinalencoder.categories_):\n        for j in values:\n            columns.append(str(categories[i] + \"_\" + j))\n\n    return pd.DataFrame(onehot.toarray(), columns=columns)\n\n\n# Apply transformation to data\ntrainx = trainx.reset_index(drop=True).join(onehot(trainx))\ntestx = testx.reset_index(drop=True).join(onehot(testx))\n\n# Drop the original categories\ntrainx = trainx.drop(categories, axis=1)\ntestx = testx.drop(categories, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training\n========\n\nFor this model, we\\'ll use a simple logistic regression model with\nelastic net for regularization in sklearn across 1000 max iterations.\nYou could still do the same bias assessment we\\'ll be carrying out here\nwith other models, because the approach we will use is a **post-hoc\napproach**, meaning it only requires the model predictions and not\naccess to the model itself.\n\nThis is in contrast to some model-specific fairness approaches that\nrequire access to the model internals.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Defining a logistic regression model\n\nmodel = LogisticRegression(penalty=\"elasticnet\", max_iter=1000, solver=\"saga\", l1_ratio=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We train the model and apply it to generate predictions on our test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Training the model with all available features\nmodel.fit(trainx, trainy)\n\n# generate 10000 predictions for 10000 train individuals\ntrain_predictions = model.predict(trainx)\nprint(\"Training accuracy: \", skm.accuracy_score(trainy, train_predictions))  # Training accuracy\n\n# generate 1000 predictions for 1000 test individuals\npredictions = model.predict(testx)\nprint(\"Test accuracy: \", skm.accuracy_score(testy, predictions))  # Test accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We notice that the train and test accuracy are all fairly good. We can\nvisualize the performance of the model further by looking at a confusion\nmatrix.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def confusionmatrix(truelabels, predictions):\n    confusion_matrix = skm.confusion_matrix(truelabels, predictions)\n    tn, fp, fn, tp = confusion_matrix.ravel()\n    print(\n        \"Sensitivity: \",\n        tp / (tp + fn),\n        \"\\nSpecificity: \",\n        tn / (tn + fp),\n        \"\\nPPV: \",\n        tp / (tp + fp),\n        \"\\nNPV: \",\n        tn / (tn + fn),\n    )\n\n    skm.ConfusionMatrixDisplay(confusion_matrix).plot()\n\n\nconfusionmatrix(testy, predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Again, we notice very solid performance. The model is correctly\nidentifying positive cases (Sensitivity), while rejecting negative ones\n(Specificity). The hospital executives probably took a look at the\nmodel, and believed that it was ready for deployment.\n\nThis performance is another overly-simplistic component of our case\nscenario. In real-world settings, and for psychiatric outcomes in\nparticular, ML rarely achieves performance this good, partly because\ntraining features are never as clearly related to outcomes as complex as\ndiagnoses of Schizophrenia or Affective Disorder. These conditions are\noften heterogeneous, meaning that individuals with the same disorder can\nhave very different symptom profiles. Symptoms of schizophrenia (such as\na lack of emotion and motivation, cognitive impairment, and even\npsychosis) can overlap with symptoms of depression. Another reason ML\noften falls short in predicting psychiatric outcomes is when these\noutcomes tend to be rare (such as in the case of suicide\n`belsher2019prediction`{.interpreted-text role=\"footcite\"}). Although\nour simulated data is fairly balanced with respect to the two diagnoses,\nin reality, Schizophrenia is much less common than Affective Disorder.\nIf collected naturally based on different patients seen at a health\nsystem, our classes would be very imbalanced.\n\nFairness Assessment\n===================\n\nMany ML algorithms deployed in the past have failed to properly address\nfairness. Perhaps the most infamous example of this is the COMPAS\nalgorithm `angwin2016machine`{.interpreted-text role=\"footcite\"} for\nprediction criminal recidivism, where Black defendants were found to\nhave higher risk scores, a higher false positive rate, and a lower false\nnegative rate compared to white defendants. In other applications of ML,\nthe lack of fairness concerns proves pervasive. An audit of three facial\nrecognition software from IBM and Microsoft found the error rate for\ndarker-skinned females to be 34% higher than for lighter-skinned males\n`buolamwini2018gender`{.interpreted-text role=\"footcite\"}. When\ntranslating into a gendered language, Google Translate skews towards\nmasculine translations for words like \"strong\" or \"doctor,\" while\nskewing feminine for words like \"beautiful\" or \"nurse.\"\n`kuczmarski2018reducing`{.interpreted-text role=\"footcite\"}\n\nThese are clear problems which we aim to avoid by performing a proper\nfairness assessment. Let\\'s do that now.\n\nQuantifying fairness\n--------------------\n\nMany fairness metrics are calculated from basic performance metrics,\nlike sensitivity, specificity, TPR, FPR, etc. There are many performance\nmetrics available, so we need to select one that is most relevant to our\ntask: decision trees like\n[this](http://www.datasciencepublicpolicy.org/our-work/tools-guides/aequitas/)\none provided by Aequitas provide an easy way to understand the use cases\nfor different metrics.\n\nAs an example, one common (and arguably most successful to date)\napplication of ML in healthcare is in the diagnosis of tumours from\nmedical images (like chest x-rays) as being cancerous or non-cancerous.\n\n**In this task, which performance metrics are most relevant?**\n\nIn this case, **false negative predictions** would probably be most\nrelevant, because when classifying tumours as being cancerous or\nnon-cancerous, we would prefer to mistake a benign tumour as being\ncancerous, as compared to the other way around. This is called a false\npositive (or misdiagnosis), and it's better than a false negative (or\nunderdiagnosis) in this specific circumstance.\n\nWhy? Because a false negative would result in a false perception that\nthe patient does not have cancer, which could delay potentially\nlife-saving treatment.\n\nThere is a study suggesting that, when diagnosing the nature of tumours\nfrom medical images, false negative rates are higher for disadvantaged\nor underserved groups defined by intersecting features (e.g.,\nlow-income, Black women).\n`seyyedkalantari2021underdiagnosis`{.interpreted-text role=\"footcite\"}\nThis is an excellent, real-world example of how the application of ML in\nhealthcare can *amplify harms*, if deployed without concern for\nfairness.\n\n*Which performance metric is most important in our case?*\n\nIn our case, the opposite would be true - a false positive or\nmisdiagnosis is more harmful for our hypothetical scenario, because it\nwould lead to unnecessary treatment for schizophrenia being\nadministered, as well as other negative impacts of a schizophrenia\ndiagnosis, such as stigma or a poorer perceived prognosis. While we want\nevery patient to be appropriately diagnosed, clinicians generally agree\nthat it is better to be misdiagnosed with an affective disorder as\ncompared to schizophrenia. Indeed, when diagnosing patients, clinicians\ntend to rule out any factors that could lead to psychosis or symptoms of\nschizophrenia, such as trauma, substance use, and affective disorder,\nwhich in some cases, can have psychotic features.\n\nSo in our example, we'd like to evaluate whether rates of misdiagnosis\nor **false positives** are the same across patient groups. False\npositive rates are calculated by dividing false positive predictions by\nall negative predictions ($FPR = \\frac{FP}{FP + TN}$)\n\nFairness metrics\n----------------\n\nWhen evaluating ML models for fairness, we typically examine the ratio\nof a given performance metric between two groups of interest, and\nwhether it is greater or less than true parity (1). The numerator in\nthis equation is the metric for the group we are interested in\nevaluating (on top), and the denominator is the same metric for the\nreference group (bottom).\n\n> $\\text{Relative Metric} = \\frac{\\text{Metric }_{\\text{group of interest}}}{\\text{Metric\n> }_{\\text{reference group}}}$\n\nIf this ratio is greater than 1, then the metric is higher in the group\nof interest vs reference.\n\nIf this ratio is less than 1, then the metric is lower in the group of\ninterest vs reference.\n\nFairness metrics themselves can be broadly divided into 3 categories\n`barocas2019fairness`{.interpreted-text role=\"footcite\"}:\n\n-   Independence: Outcomes should be evenly distributed between\n    subgroups\n-   Separation: Errors should be evenly distributed between subgroups\n-   Sufficiency: Given a prediction, an individual has an equal\n    likelihood of belonging to any subgroup\n\nIn our case, we calculate the relative false positive rate via the\ncategory of separation, by comparing false positive or misdiagnosis\nrates between our group of interest (Black men) and our reference group\n(white men).\n\n> $\\text{Relative FPR} = \\frac{\\text{FPR}_{\\text{Black men}}}{\\text{FPR}_{\\text{white\n> men}}}$\n\nNote that there are some subjective decisions we are making in our\nfairness assessment. First, we have chosen white men as our reference\ngroup, but other groups might be justified here. For instance, if we are\ninterested in racialization related to Black groups specifically, we\nmight choose another racialized group as our reference (e.g., Hispanic\nmen). If we are interested in the impact of gender, we may choose Black\nwomen as our reference group.\n\nAnother decision is related to the metric -- we could have also\nconsidered some other performance metrics. Selection rate, for example,\nrefers to the proportion of positive predictions between the two groups:\n\n> $\\text{Selection rate}= \\frac{TP + FP}{n}$\n\nBecause we are primarily interested in misdiagnosis of schizophrenia and\nnot the total number of schizophrenia diagnoses (which indeed may be\nhigher in certain demographic groups), false positive rate is likely a\nbetter fit for our task.\n\nCritically, there are no readily-agreed upon definitions and\nunderstandings of fairness. There are over 70 definitions of fairness,\nmany of which conflict with each other, making it **impossible to\nsimultaneously satisfy all possible metrics for fairness.**\n\nRacial bias\n-----------\n\nIn order to perform a fairness assessment, there are a few key things we\nneed to do:\n\n**1) Which population might be unfairly affected by this model?**\n\nBased on the research, we have defined this group to be Black men. In\nother words, we are interested in a particular intersection of race and\nsex. In fairness terms, we define race and sex to be **sensitive\nvariables** - features that we want to ensure our model isn\\'t being\ndiscriminatory against.\n\nWe\\'ll start with an assessment purely on the basis of Race as a\nsensitive variable (How might all Black individuals be affected by an\nunfair model?) and then add in sex (How might Black men be affected by\nunfair model?) in order to demonstrate the value of an intersectional\napproach.\n\n**2) What is fairness in this context?**\n\nWe\\'ve also determined a fairness metric, which quantifies the exact\nnature of the discrimination which groups may face and that we seek to\nminimize. As we mentioned, various fairness metrics may be relevant, but\nwe are primarily concerned about misdiagnosing individuals with\naffective disorder as having schizophrenia. We examine false positive\nrates (i.e., false diagnoses of schizophrenia), with the help of\nFairlearn.\n\n**3) How do we compare populations?**\n\nFinally, the performance of a ML model cannot be analyzed in isolation,\nbut rather in comparison between different demographic subgroups. In\norder to do this, a reference group is required.\n\nAs we explained, we will use white individuals as a reference group, but\nthis assessment could be conducted with any reference group, or compared\nto an overall score across all groups. It is important to note that both\napproaches can be problematic: there are concerns that using white\ngroups as reference groups centers their perspectives or experiences and\nrepresents other groups as *other* or outliers, or that using overall\naverages may mask disparities (especially intersectional ones) and\nignore important historical and social context. Keeping this in mind, we\nshould carefully consider which group we use as a reference and the\nimplications of this choice.\n\nHere, we apply Fairlearn to compare performance in the test set among\nthe different racial groups, with a focus on evaluating false positive\nrate ratio.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def f(truelabels, predictions):\n    # Define race to be the sensitive variable\n    sensitive = test.Race\n\n    # Define a MetricFrame using a FPR of the defined sensitive features, using the true labels and\n    # predictions\n    fmetrics = MetricFrame(\n        metrics=false_positive_rate,\n        y_true=truelabels,\n        y_pred=predictions,\n        sensitive_features=sensitive,\n    )\n\n    # Compute the Relative FPR relative to white individuals.\n    results = pd.DataFrame(\n        [fmetrics.by_group, fmetrics.by_group / fmetrics.by_group.White],\n        index=[\"FPR\", \"Relative FPR\"],\n    )\n    return results\n\n\nf(testy, predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the table above, the first row shows the metric (false positive\nrates), while the second row shows the ratio of the metric between\nspecified group and reference. These values are all expressed as\npercentages.\n\nSome observations to note:\n\n-   The FPR are fairly low across the board, which are due to our use of\n    simulated data (as we explained, false positive rates in real-world\n    data are likely to be much higher)\n-   However, **disparities in performance are emerging**: Black patients\n    have a relative FPR of 2.9, which means they are being misdiagnosed\n    with schizophrenia when they have affective disorder at a rate that\n    is 2.9x higher than those who are white. This is quite concerning.\n-   We see that Hispanic individuals are also poorly affected by this\n    model, with a relative FPR of 1.3 (but you should note that this is\n    not an effect that has been explicitly noted in the literature and\n    is likely an artifact of our simulated data)\n-   The white population has a relative FPR of 1 - this makes since they\n    are our reference population, and other other group is being\n    compared against the FPR for whites.\n\nIntersectional bias\n===================\n\nHowever, we suspect this bias might only extend to identities defined by\nthe intersecting features of sex and race (i.e., Black men\n`gara2019naturalistic`{.interpreted-text role=\"footcite\"}).\n\nLet\\'s repeat this assessment then.\n\nTo do this with Fairlearn, we define sensitive features as the\nintersection of race and sex.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def intersectionalf(truelabels, predictions):\n    # Sensitive features are now the intersection of race and sex\n    sensitive = pd.DataFrame(np.stack([test.Race, test.Sex], axis=1), columns=[\"Race\", \"Sex\"])\n\n    fmetrics = MetricFrame(\n        metrics=false_positive_rate,\n        y_true=truelabels,\n        y_pred=predictions,\n        sensitive_features=sensitive,\n    )\n\n    results = pd.DataFrame(\n        [fmetrics.by_group, fmetrics.by_group / fmetrics.by_group.White.Male],\n        index=[\"FPR\", \"Relative FPR\"],\n    )\n    return results\n\n\nintersectionalf(testy, predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What do we notice?\n\n-   Both the FPR and Relative FPR of Black men is much higher than that\n    of Black women and men combined. Black men are now misdiagnosed at a\n    rate that is almost 5.18x that of White men. This suggests that\n    Black men are specifically being unfairly treated by this model.\n-   Black women still have a higher rate of misdiagnosis than white men,\n    but we see now that the Relative FPR for this group is actually\n    lower than the Black population as a whole. This provides further\n    support for the notion that Black men are an intersectional group\n    that is being unfairly harmed.\n-   Crucially, this insight is something we would have completely missed\n    out on without looking at this problem through an intersectional\n    lens\n\nWe can take a look at the data to get an idea about what might be going\non. First, we examine the frequency of diagnoses, stratified by\nintersectional groups.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "m = sns.FacetGrid(train, row=\"Sex\", col=\"Race\")\nm.map(sns.histplot, \"Diagnosis\", discrete=True, shrink=0.8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that the frequency of the two diagnoses (of affective disorder or\n0 and schizophrenia or 1) are fairly similar among all intersectional\nsubgroups, except Black men, who have a much higher rate of\nschizophrenia diagnosis.\n\nFairness through Unawareness?\n=============================\n\nIn our data, Black men are less likely to be diagnosed with affective\ndisorder, and more likely to be diagnosed with schizophrenia than other\ngroups (and almost two times as likely as white men). Perhaps the model\nis picking up on this trend, which is contributing to bias. If we remove\nrace, the classifier will no longer have access to this information\nduring training.\n\nThis is an approach that is commonly termed **fairness through\nunawareness.** Specifically, it refers to the notion that a model that\ndoes not have access to a given feature when making predictions cannot\nbe unfair with respect to that feature.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define and drop race-related variables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "race_cat = [\"Race_Asian\", \"Race_Black\", \"Race_Hispanic\", \"Race_White\"]\ntrainx_norace = trainx.drop(race_cat, axis=1)\ntestx_norace = testx.drop(race_cat, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we\\'ve dropped the variables, we\\'ll train a second model that\nis identical to the first one, except it no longer uses Race as a\nfeature.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Define and train a second model\nmodel2 = LogisticRegression(penalty=\"elasticnet\", max_iter=1000, solver=\"saga\", l1_ratio=1)\nmodel2 = model2.fit(trainx_norace, trainy)\n\ntrain_predictions = model2.predict(trainx_norace)\nprint(\"Training accuracy: \", skm.accuracy_score(trainy, train_predictions))  # Training accuracy\n\npredictions = model2.predict(testx_norace)\nprint(\"Test accuracy: \", skm.accuracy_score(testy, predictions))  # Test accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We\\'ll note that the accuracy is still solid. It has decreased slightly,\nwhich makes sense given that our model has access to fewer features with\nwhich to maximize predictive accuracy.\n\nThat said, our most important objective is to analyze whether fairness\nhas increased as a result of our change, so let\\'s perform another bias\nassessment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "f(testy, predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Oh yikes - by removing race from our model, the FPR for the Black\npopulation has increased to 3.8x that of our reference group.\n\nDoes intersectional fairness show a similar trend?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "intersectionalf(testy, predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we analyze this problem through an intersectional lens, we notice\nthat the relative FPR has increased even further, and Black men are now\nbeing misdiagnosed at a rate that is 7x higher than white men.\n\n**After we have observed this finding, is it a good idea to implement\nour model? Probably not.**\n\nAfter all, the potential harm related to an unfair model like this one\ndoes not stop at the fairness assessment - it can manifest in impactful\nand pervasive ways in the systems where it is potentially deployed. A\nBlack man misdiagnosed with schizophrenia with the help of a ML model\nmay become more distrustful towards healthcare in the future. At any\nfuture visits, this distrust might manifest in certain types of\nbehaviours (e.g., an increased sense of tension) that could be\ninterpreted as further evidence of schizophrenia, ultimately\ncontributing to further misdiagnosis. This type of feedback is extremely\ndetrimental, as **algorithms reinforce and propagate the unfairness**\nencoded within the data that is representative of society\\'s own\ndiscriminatory practices.\n\nThis begs the question though - if unfair predictions made by an\nalgorithm are capable of causing harm, should we attempt to remove this\nbias from our model? After all, there are methods that exist that allow\nus to transform our data or modify our training algorithm in a way that\nwould make our model fairer with respect to the fairness metrics we are\nusing.\n\nIn answering this, it\\'s important to remember that the bias we are\nseeing in our model are a reflection of systemic biases that exist in\nreal life. While the real life biases that lead to patient misdiagnosis\nare certainly problematic, detecting these biases in our model isn\\'t\nnecessarily a bad thing. In this case scenario, where our main goal is\nactually to better understand the model, the presence and quantification\nof these biases are actually very helpful because they enable us to\nunderstand the systemic biases that have been encoded into our data.\n**In other words, our model can be insightful \\*because\\* it captures\nthese harmful real-world biases.**\n\nTo this end, attempting to remove the biases from our dataset would be\ndetrimental to our aim. Instead, we should dig a little bit deeper to\nreflect and analyze some of the systemic factors that could be\nunderlying our findings.\n\nWhy is our model still biased?\n==============================\n\nWhy has removing race information from our training data not fixed our\nproblem?\n\nRecall that there is much evidence to support **diagnostic bias**, or\nthe tendency for clinicians to under-emphasize depressive symptoms and\nover-emphasize psychotic symptoms when assessing black men. But one\ninteresting 2004 study `arnold2004ethnicity`{.interpreted-text\nrole=\"footcite\"} shows that this effect is not necessarily due to the\nappearance of race. In this study, the researchers examined whether\nblinding clinicians to a patient's race would remove this diagnostic\nbias. Black and white patients presenting to the hospital with psychosis\nwere evaluated with structured rating scales, and this evaluation was\ntranscribed. Cues indicating the patient's race were removed from the\ntranscription. Regardless of whether clinicians were blinded or\nunblinded to the patient's race, they still rated the black men as\nhaving increased psychotic symptoms.\n\n**So what is going on here?**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# It\u2019s likely that race is **associated with other factors that are relevant to misdiagnosis**. The\n# diagnostic bias (or a tendency to overemphasize symptoms of schizophrenia and under-emphasize\n# depressive symptoms) may be related to socio-environmental factors, for example, Black men with\n# depression facing more barriers to receiving mental healthcare, which results in more severe\n# illness when finally assessed. Growing up in low-income or stressful environments, or early\n# exposure to malnourishment and trauma, can also lead to more severe impairment in daily and\n# cognitive functioning. Black patients may additionally face racialization and poor treatment in\n# health settings, leading them to exhibit paranoia, tension, or distrust at assessment (especially\n# if being assessed by a white clinician). The diagnostic instruments we have are also likely\n# culturally biased, having been developed on mostly white populations, making it difficult to pick\n# up on symptoms of depression in Black patients or men, in particular. (This article\n# :footcite:ct:`herbst2022schizophrenia`presents a nice overview of these various concerns)\n#\n# Unfortunately then, it\u2019s not as simple as removing race from the equation, because of these\n# **persisting systemic biases which are related to misdiagnosis.** And these biases are reflected\n# in the training data, since the socio-environmental and clinical factors relevant for\n# misdiagnosis are also associated with race. The model is picking up on these associations,\n# despite not having access to the race of each patient.\n#\n# Now that we have evidence for intersectional bias in our model, we could explore some of the\n# training features that might underlie this bias. In fact, there is a very recent study by\n# :footcite:ct:`banerjee2021readiang` which takes this point a bit further. In this study,\n# researchers found that deep learning models trained on medical images, like chest X-rays,\n# performed well at predicting the patient\u2019s race, despite not having access to any racial\n# information. The researchers examined some reasonable explanations for how this was even\n# possible, such as minor anatomical differences between racial groups, cumulative effects of\n# racialization or stress, and even image quality between health systems, and none were supported.\n# So there must be some way that models pick up on race that not even us humans can understand.\n#\n# Feature evaluation\n# ~~~~~~~~~~~~~~~~~~\n#\n# In sum, the tendency for Black men to be misdiagnosed with schizophrenia is not simply a result\n# of clinician or interpersonal bias, but likely reflects systemic factors (e.g., barriers to care\n# leading to severe illness at assessment, expression of emotional and cognitive symptoms of\n# depression, experiences of racialization leading to greater paranoia or distrust).\n#\n# These factors may be reflected in other features in the simulated data, which are related to\n# race, and contribute to bias.\n#\n# We can explore relations among the features in different ways, but one option is to see how\n# various features are related to schizophrenia in the training set, and then explore these\n# features in groups with affective disorder in the test set.\n#\n# The choice of which features to consider is subjective and can be based on existing research or\n# empirical observation (or both). In our case, we'll use the first few features identified as\n# being important for prediction in our ML model.\n#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Gets the weights associated with each feature, and scales them from\n0-100\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "weights = pd.DataFrame(model.coef_[0], trainx.columns, columns=[\"Weight\"])\nscaler = MinMaxScaler((0, 100))\nscaled_weights = pd.DataFrame(\n    scaler.fit_transform(abs(weights)), trainx.columns, columns=weights.columns\n)\nscaled_weights.sort_values(by=[\"Weight\"], ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When we examine features that are most important for prediction in our\nfirst model (i.e., the model that included race), it indeed shows that\nalthough race is among the important features, there are also other\nfeatures related to clinical presentation that are contributing to\npredicting the diagnosis or outcome.\n\nAs mentioned, we will probe into potential factors underlying\nmisdiagnosis of individuals with affective disorder by examining how\nthese features are related to diagnosis in the training set. Then, we\nwill compare individuals with affective disorder on these features in\nthe test set, stratified by intersectional group.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Reloading the data for analysis\ntrain = data.loc[data.dataset == \"train\"].drop(\"dataset\", axis=1)\ntest = data.loc[data.dataset == \"test\"].drop(\"dataset\", axis=1)\n\ntrain[\"Diagnosis\"] = train[\"Diagnosis\"].astype(int)\ntest[\"Diagnosis\"] = test[\"Diagnosis\"].astype(int)\n\n# Create new intersect column for plotting\ntest[\"intersect\"] = \"\"\ntest.loc[(test[\"Sex\"] == \"Male\") & (test[\"Race\"] == \"White\"), \"intersect\"] = \"WhiteM\"\ntest.loc[(test[\"Sex\"] == \"Male\") & (test[\"Race\"] == \"Black\"), \"intersect\"] = \"BlackM\"\ntest.loc[(test[\"Sex\"] == \"Male\") & (test[\"Race\"] == \"Hispanic\"), \"intersect\"] = \"HispanicM\"\ntest.loc[(test[\"Sex\"] == \"Male\") & (test[\"Race\"] == \"Asian\"), \"intersect\"] = \"AsianM\"\ntest.loc[(test[\"Sex\"] == \"Female\") & (test[\"Race\"] == \"White\"), \"intersect\"] = \"WhiteF\"\ntest.loc[(test[\"Sex\"] == \"Female\") & (test[\"Race\"] == \"Black\"), \"intersect\"] = \"BlackF\"\ntest.loc[(test[\"Sex\"] == \"Female\") & (test[\"Race\"] == \"Hispanic\"), \"intersect\"] = \"HispanicF\"\ntest.loc[(test[\"Sex\"] == \"Female\") & (test[\"Race\"] == \"Asian\"), \"intersect\"] = \"AsianF\"\n\nfig, axs = plt.subplots(2, 1, figsize=(8, 10))\nsns.barplot(x=\"Diagnosis\", y=\"Rumination\", data=train, ax=axs[0])\naxs[0].set_title(\"Rumination vs Diagnosis (train)\")\n\nsns.barplot(\n    x=\"intersect\",\n    y=\"Rumination\",\n    data=test.loc[test.Diagnosis == 0],\n    ax=axs[1],\n    order=[\n        \"BlackF\",\n        \"WhiteF\",\n        \"HispanicF\",\n        \"AsianF\",\n        \"BlackM\",\n        \"WhiteM\",\n        \"HispanicM\",\n        \"AsianM\",\n    ],\n)\naxs[1].set_title(\"Rumination vs Intersect across groups with AD (test)\")\n\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The first graph (left) shows average rumination scores stratified by\ndiagnosis. Rumination is a cognitive feature of depression, referring to\nrepetitive, persistent thinking about the depressive episode. As you can\nsee, rumination is more common in affective disorder than schizophrenia,\nwhich is consistent with clinical trends.\n\nThe second graph (right) shows rumination among individuals with\naffective disorder in the test set. You can see that rumination is lower\namong men than women, but also that **Black men as compared to white men\nare less likely to report rumination**, which are the two groups we\\'re\ncomparing. As such, this could be a potential factor contributing to\nmisdiagnosis, leading to higher false positive rates in Black men. Of\ncourse, other groups may have lower rumination scores relative to white\nmen as well (e.g., Hispanic men), but the model is picking up on trends\nrelated to a variety of features, so we can take a look at one more.\n\nAnother important feature is tension, which is a symptom of\nschizophrenia. We can carry out a similar exploration to examine how\ntension is reported by individuals with affective disorder defined by\nintersecting features of sex and race, as compared to schizophrenia.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(2, 1, figsize=(8, 10))\nsns.barplot(x=\"Diagnosis\", y=\"Tension\", data=train, ax=axs[0])\naxs[0].set_title(\"Tension vs Diagnosis (train)\")\n\nsns.barplot(\n    x=\"intersect\",\n    y=\"Tension\",\n    data=test.loc[test.Diagnosis == 0],\n    ax=axs[1],\n    order=[\n        \"BlackF\",\n        \"WhiteF\",\n        \"HispanicF\",\n        \"AsianF\",\n        \"BlackM\",\n        \"WhiteM\",\n        \"HispanicM\",\n        \"AsianM\",\n    ],\n)\naxs[1].set_title(\"Tension vs Intersect across groups with AD (test)\")\n\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the training set, people with schizophrenia are more likely to report\ntension at their clinical assessment (left graph). Again, looking at how\nBlack men with affective disorder compare to White men on this\nparticular feature in the test set, they are more likely to report\ntension at the clinical interview. But it\\'s interesting to see that\nother groups also report high tension relative to our reference group\n(white men).\n\nSo it\\'s not the only factor that\\'s potentially contributing to\nmisdiagnosis, but it could be one of the factors that explain the high\nfalse positive rates in Black men with affective disorder (as well as in\nAsian men or Hispanic women, for example).\n\nThe goal of this tutorial was to show one potential way to probe into\nfactors potentially underlying biased performance, but there are other\nways, many of which are not limited to quantitative means. For example,\nethnographic techniques can provide a more comprehensive and meaningful\nunderstanding of the social, systemic, and political factors that\ncontribute to inequities, and they can be particularly powerful at\nelucidating the contextual factors leading to training data bias.\n\nConclusions\n===========\n\nReporting to our stakeholders\n-----------------------------\n\nGetting back to our hypothetical scenario, what do we report back to our\nstakeholders following our intersectional bias assessment?\n\nWe conclude that although the model shows excellent performance overall,\nit may underserve certain demographic groups, like Black men, and it\nshould not be deployed without further assessment by clinicians and\nfurther research into the factors contributing to bias. The hospital\nshould consider some targeted interventions (e.g., further assessment\nfor Black men and other intersectional groups, especially those\nreporting or displaying less rumination or more tension at clinical\nassessments).\n\nOverall, the model should not be deployed without further assessment by\nclinicians or intervention.\n\nSome final points\n-----------------\n\nResearchers have developed algorithmic methods to mitigate the\nfairness-related harms that may result from ML models (by adjusting\nmodel parameters or modifying training data), **but this does not do\nanything to address the systemic factors** contributing to bias. For\nexample, if we fix our model to reduce false positive predictions in\nBlack men, will this increase their access to care or treatment? Will it\nhelp clinicians better differentiate between symptoms of depression or\nschizophrenia in these groups? As we have demonstrated in our case\nscenario, the problem is more nuanced and the solution is much more\ncomplex, **requiring collaboration between researchers, clinicians, and\npublic health or policy administrators**. We need more research into\nthese issues and interventions that can address them. For example, some\nevidence suggests that forcing clinicians to carry out more consistent\nand structured assessments can reduce diagnostic bias (though not\ncompletely). Put differently, machine learning systems are of a\n**sociotechnical nature** (see also our\n`user guide<concept_glossary>`{.interpreted-text role=\"ref\"} and\n:footcite`selbst2019fairness`{.interpreted-text role=\"ct\"}).\n\nThe point is **not** that we shouldn't be using ML to automate or inform\nclinical tasks (this will likely happen whether we like it or not).\nRather, ML can potentially help us **better understand the potential\nhealth inequities present within a health system** (many of which we\nmight not catch because our own biases can prevent us from seeing and\nthinking about the inequities). This underscores the potential of ML to\nidentify contributing features that warrant more research and to improve\ncurrent clinical practices.\n\nReferences\n==========\n\n::: {.footbibliography}\n:::\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}